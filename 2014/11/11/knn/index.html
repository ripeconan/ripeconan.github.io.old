
 <!DOCTYPE HTML>
<html >
<head>
  <meta charset="UTF-8">
  <meta name="baidu_union_verify" content="d1952c66cf48912e21c18c7c581f382a">
  <meta name="360-site-verification" content="67fbcc5a67f4c65c057315b28fa0b2c8" />
<meta name="google-site-verification" content="2GzxQ0VtXwTSUdmGm6DzcmhTzM_I9QmzCb_pzpMzD88" />
  
    <title>knn 算法简介和三种实现方式 | ripeconan</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=3, minimum-scale=1">
    
    <meta name="author" content="ripeconan">
    
    <meta name="description" content="knn 算法介绍
knn 是 k-nearest neighbour 的缩写，又名 k 邻近。knn 算法的核心思想是“近朱者赤，近墨者黑”。拿一个电子商务系统举例来说，该系统在为你做商品推荐的时候，需要根据和你具有相似特征的人群的购买记录来为你推荐。它的理由就是：如果他们和你是类似的人的话，那么他">
    
    
    
    
    <link rel="alternate" href="atom.xml" title="ripeconan" type="application/atom+xml">
    
    
    <link rel="icon" href="/img/favicon.ico">
    
    
    <link rel="stylesheet" href="/css/style.css" type="text/css">

    
    <script>
        var _hmt = _hmt || [];
        (function() {
            var hm = document.createElement("script");
            var _bdId ='63d745e76199bc2f1546dacc46fc8127';
             hm.src = "//hm.baidu.com/hm.js?" + _bdId;
             var s = document.getElementsByTagName("script")[0]; 
             s.parentNode.insertBefore(hm, s);
        })();
    </script>
     
</head>

  <body>
    <header>
      <div>
		
			<div id="textlogo">
				<h1 class="site-name"><a href="/" title="ripeconan">ripeconan</a></h1>
				<a class="blog-motto"></a>
			</div>
			<div class="navbar"><a class="navbutton navmobile" href="#" title="Menu">
			</a></div>
			<nav class="animated">
				<ul>
					<ul>
					 
						<li><a href="/">首页</a></li>
					
						<li><a href="/archives">归档</a></li>
					
					<li>
					
                                            <form class="search" action=http://zhannei.baidu.com/cse/search target="_blank">
                                            <label>Search</label>
                                        <input name="s" type="hidden" value= 1569724209730504620 ><input type="text" name="q" size="30" placeholder="Search"><br>
					
					
				</ul>
                            </nav>			
</div>

    </header>
    <div id="container">
      <div id="main" class="post" itemscope itemprop="blogPost">
	<article itemprop="articleBody"> 
		<header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2014/11/11/knn/" title="knn 算法简介和三种实现方式" itemprop="url">knn 算法简介和三种实现方式</a>
  </h1>
  <p class="article-author">By
    
      <a href="http://ripeconan.com" title="ripeconan">ripeconan</a>
    </p>
  <p class="article-time">
    <time datetime="2014-11-10T16:00:00.000Z" itemprop="datePublished">Nov 11 2014</time>
    Updated:<time datetime="2014-11-11T04:12:28.000Z" itemprop="dateModified">Nov 11 2014</time>
    
  </p>
</header>
	<div class="article-content">
		
		
		<div id="toc" class="toc-article">
			<strong class="toc-title">Contents</strong>
		<ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#knn_算法介绍"><span class="toc-number">1.</span> <span class="toc-text">knn 算法介绍</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#使用自己编写的代码实现_knn_算法"><span class="toc-number">2.</span> <span class="toc-text">使用自己编写的代码实现 knn 算法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#数据集介绍"><span class="toc-number">2.1.</span> <span class="toc-text">数据集介绍</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#距离矩阵定义"><span class="toc-number">2.2.</span> <span class="toc-text">距离矩阵定义</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#knn_函数的定义"><span class="toc-number">2.3.</span> <span class="toc-text">knn 函数的定义</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#正确率的计算"><span class="toc-number">2.4.</span> <span class="toc-text">正确率的计算</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#使用_class_包实现_knn_算法"><span class="toc-number">3.</span> <span class="toc-text">使用 class 包实现 knn 算法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#数据准备"><span class="toc-number">3.1.</span> <span class="toc-text">数据准备</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#使用_class::knn_和_class::knn1_进行分类"><span class="toc-number">3.2.</span> <span class="toc-text">使用 class::knn 和 class::knn1 进行分类</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#使用_class::knn-cv_进行分类"><span class="toc-number">3.3.</span> <span class="toc-text">使用 class::knn.cv 进行分类</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#使用_caret_包实现_knn_算法"><span class="toc-number">4.</span> <span class="toc-text">使用 caret 包实现 knn 算法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#使用_createDataPartition_函数划分训练集和测试集"><span class="toc-number">4.1.</span> <span class="toc-text">使用 createDataPartition 函数划分训练集和测试集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#使用_knn3_函数进行分类预测"><span class="toc-number">4.2.</span> <span class="toc-text">使用 knn3 函数进行分类预测</span></a></li></ol></li></ol>
		</div>
		
		<h2 id="knn_算法介绍">knn 算法介绍</h2>
<p>knn 是 <strong>k-nearest neighbour</strong> 的缩写，又名 k 邻近。knn 算法的核心思想是“近朱者赤，近墨者黑”。拿一个电子商务系统举例来说，该系统在为你做商品推荐的时候，需要根据和你具有相似特征的人群的购买记录来为你推荐。它的理由就是：如果他们和你是类似的人的话，那么他们想要购买的东西，想必你也是需要买的，所以推荐给你。那么如何定义和你具有相似特征的人呢？这些特征可能是性别、年龄、地域、浏览记录、购买记录、收藏记录等各种数据。对于这些不同类别的数据，无论是分类数据，还是数值型数据，我们都有若干种方法来定义两个样本点之间的距离，比如最常见的<strong>欧氏距离，曼哈顿距离</strong>等。通过利用合适的方法来度量距离，我们可以得到谁和你是类似的人。那么他们喜欢的东西，系统经过排序，将会将结果展现给你。</p>
<p>除了使用哪种距离以外，我们还需要关注的是，需要综合你周围的几个人的意见来为你做推荐？是需要少数几个人就够，还是需要很多人的综合意见？这就是 knn 算法中的参数 k 的含义。通常情况下，选择不同的 k 会使得我们的算法的表现有好有坏，我们需要对 k 经过多种尝试，来决定到底使用多大的 k 来作为最终参数。</p>
<p>对于 knn 算法的实现， R 语言中已经有了很多种方式。以下我们介绍其中三种。第一种是通过自己编写代码来实现 knn 算法，其代码来自于 <em>&lt;机器学习:实用案例解析&gt;</em> 一书。另外两种分别借助 R 语言中的两个包，分别是 <strong>class</strong> 包和 <strong>caret</strong> 包。</p>
<hr>
<h2 id="使用自己编写的代码实现_knn_算法">使用自己编写的代码实现 knn 算法</h2>
<h3 id="数据集介绍">数据集介绍</h3>
<p>首先我们使用的是来自于 &lt;机器学习:实用案例解析&gt; 自带的数据，可以点击<a href="https://github.com/johnmyleswhite/ML_for_Hackers/blob/master/10-Recommendations/data/example_data.csv" target="_blank" rel="external">这里</a>下载</p>
<p>首先我们读取数据集，并进行查看</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">data &lt;- read.csv(<span class="string">"example_data.csv"</span>)</div><div class="line">str(data)</div></pre></td></tr></table></figure>



<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="preprocessor">## 'data.frame':	100 obs. of  3 variables:</span></div><div class="line"><span class="preprocessor">##  $ X    : num  2.37 3.18 2.16 4.6 3.33 ...</span></div><div class="line"><span class="preprocessor">##  $ Y    : num  5.4 4.39 5.34 3.87 6.43 ...</span></div><div class="line"><span class="preprocessor">##  $ Label: int  0 0 0 0 0 0 0 0 0 0 ...</span></div></pre></td></tr></table></figure>



<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">summary(data)</div></pre></td></tr></table></figure>



<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">##        <span class="tag">X</span>                <span class="tag">Y</span>             <span class="tag">Label</span>    </div><div class="line">##  <span class="tag">Min</span>.   <span class="pseudo">:0</span><span class="class">.7853</span>   <span class="tag">Min</span>.   <span class="pseudo">:3</span><span class="class">.195</span>   <span class="tag">Min</span>.   <span class="pseudo">:0</span><span class="class">.0</span>  </div><div class="line">##  1<span class="tag">st</span> <span class="tag">Qu</span>.<span class="pseudo">:3</span><span class="class">.0829</span>   1<span class="tag">st</span> <span class="tag">Qu</span>.<span class="pseudo">:5</span><span class="class">.134</span>   1<span class="tag">st</span> <span class="tag">Qu</span>.<span class="pseudo">:0</span><span class="class">.0</span>  </div><div class="line">##  <span class="tag">Median</span> <span class="pseudo">:3</span><span class="class">.9015</span>   <span class="tag">Median</span> <span class="pseudo">:6</span><span class="class">.067</span>   <span class="tag">Median</span> <span class="pseudo">:0</span><span class="class">.5</span>  </div><div class="line">##  <span class="tag">Mean</span>   <span class="pseudo">:3</span><span class="class">.9740</span>   <span class="tag">Mean</span>   <span class="pseudo">:6</span><span class="class">.097</span>   <span class="tag">Mean</span>   <span class="pseudo">:0</span><span class="class">.5</span>  </div><div class="line">##  3<span class="tag">rd</span> <span class="tag">Qu</span>.<span class="pseudo">:4</span><span class="class">.7370</span>   3<span class="tag">rd</span> <span class="tag">Qu</span>.<span class="pseudo">:7</span><span class="class">.013</span>   3<span class="tag">rd</span> <span class="tag">Qu</span>.<span class="pseudo">:1</span><span class="class">.0</span>  </div><div class="line">##  <span class="tag">Max</span>.   <span class="pseudo">:7</span><span class="class">.0872</span>   <span class="tag">Max</span>.   <span class="pseudo">:9</span><span class="class">.308</span>   <span class="tag">Max</span>.   <span class="pseudo">:1</span><span class="class">.0</span></div></pre></td></tr></table></figure>

<p>可以看到该数据集有100个样本点，每个样本点有两个数值型特征，并且其数据分布范围相差不大。第三列是这些样本所归属的类，但是该列现在仍然是 int 类型</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">table(data$Label)</div></pre></td></tr></table></figure>



<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="preprocessor">## </span></div><div class="line"><span class="preprocessor">##  0  1 </span></div><div class="line"><span class="preprocessor">## 50 50</span></div></pre></td></tr></table></figure>

<p>归属于 0 和 1 这两类的样本各为 50 个。由于只有两个特征，我们可以使用 ggplot2 包来对这些样本点进行可视化。很显然，X 和 Y 用来确定样本点的位置，而 Label 用来给点上色。</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">library</span>(ggplot2)</div><div class="line">visual &lt;- ggplot(data, aes(X,Y,colour = factor(Label)))</div><div class="line">visual + geom_point() + scale_colour_brewer(palette = <span class="string">"Set1"</span>)</div></pre></td></tr></table></figure>

<p><img src="/img/knn/colour.png" alt="colour">  </p>
<p>可以看到，不同 Label 的点相对较为分离，不过仍有一些交叉。我们使用 knn 算法，可能会得到相对不错的效果。</p>
<hr>
<h3 id="距离矩阵定义">距离矩阵定义</h3>
<p>由于数据在二维平面上作图得到良好的感官效果，我们可以顺理成章的使用二维的欧氏距离来定义两个样本点之间的距离。在下面的代码中，我们给 distance.matrix 函数传入一个数据框 <code>df</code>。该函数会先创建一个 <code>nrow(df)</code> 行 <code>nrow(df)</code> 列的方阵，并使用循环遍历方阵中的每一个元素，将其赋值为对应位置两个样本点的欧氏距离。该函数最后返回距离矩阵。矩阵的第 <code>(i,j)</code> 位置的值，就是第 i 个样本点和第 j  个样本点的欧式距离。</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">distance.matrix &lt;- <span class="keyword">function</span>(df)</div><div class="line">{</div><div class="line">  distance &lt;- matrix(rep(<span class="literal">NA</span>, nrow(df) ^ <span class="number">2</span>), nrow = nrow(df))</div><div class="line">  </div><div class="line">  <span class="keyword">for</span> (i <span class="keyword">in</span> <span class="number">1</span>:nrow(df))</div><div class="line">  {</div><div class="line">    <span class="keyword">for</span> (j <span class="keyword">in</span> <span class="number">1</span>:nrow(df))</div><div class="line">    {</div><div class="line">      distance[i, j] &lt;- sqrt((df[i, <span class="string">'X'</span>] - df[j, <span class="string">'X'</span>]) ^ <span class="number">2</span> + (df[i, <span class="string">'Y'</span>] - df[j, <span class="string">'Y'</span>]) ^ <span class="number">2</span>)</div><div class="line">    }</div><div class="line">  }</div><div class="line"></div><div class="line">  <span class="keyword">return</span>(distance)</div><div class="line">}</div></pre></td></tr></table></figure>

<hr>
<h3 id="knn_函数的定义">knn 函数的定义</h3>
<p>有了距离矩阵，我们对任意的一个样本点，比如说第 i 个样本点，就能知道其余所有点和它的距离了。那么我们自然也可以对这些距离排个序，并找出离 i 最近的 k 个样本点来。这里需要注意的是，由于样本点 i 和自身的距离永远为零，那么我们在使用最近的 k 个邻居来对其进行预测时，当然不能把它自己给算进去了。所以距离最小的这个自身的点，我们用不上。那么有了以上的思路以后，我们就有了下面的函数，该函数用来对样本点 i 找出和其最近的 k 个样本来(不包括它自己)，并返回这些样本点的位置。</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">k.nearest.neighbors &lt;- <span class="keyword">function</span>(i, distance, k = <span class="number">5</span>)</div><div class="line">{</div><div class="line">  <span class="keyword">return</span>(order(distance[i, ])[<span class="number">2</span>:(k + <span class="number">1</span>)])</div><div class="line">}</div></pre></td></tr></table></figure>

<p>有了以上两个函数做铺垫，我们可以定义最后的 knn 函数了。该函数有两个参数，第一个是数据框，第二个是选择的最近邻居个数 k。当数据框进入 knn 函数后，我们先使用 <code>distance.matrix</code> 函数计算出其距离矩阵。然后我们使用 <code>k.nearest.neighbors</code> 函数，对数据框中的每一个样本，找出和其最近的 k 个样本点，然后计算这 k 个样本点的 Label 均值。由于 Label 只有 1 和 0 两个值，那么近邻中取哪个值的样本点占得比例高，Label 的均值就应该在 0.5 的基础上向着相应值靠拢，这就是下面利用 <code>ifelse</code> 函数对样本点的 Label 进行预测的依据这种决策方法也叫做 <strong>majority vote</strong>，或者叫<strong>草根民主</strong>。最后，knn 函数返回所有样本点的一个 Label 预测。我们可以用该预测和所有点的 Label 真实值作比较，看看我们的算法的正确率如何。</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">knn &lt;- <span class="keyword">function</span>(df, k = <span class="number">5</span>)</div><div class="line">{</div><div class="line">  distance &lt;- distance.matrix(df)</div><div class="line">  </div><div class="line">  predictions &lt;- rep(<span class="literal">NA</span>, nrow(df))</div><div class="line">  </div><div class="line">  <span class="keyword">for</span> (i <span class="keyword">in</span> <span class="number">1</span>:nrow(df))</div><div class="line">  {</div><div class="line">    indices &lt;- k.nearest.neighbors(i, distance, k = k)</div><div class="line">    predictions[i] &lt;- ifelse(mean(df[indices, <span class="string">"Label"</span>]) &gt; <span class="number">0.5</span>, <span class="number">1</span>, <span class="number">0</span>)</div><div class="line">  }</div><div class="line">  </div><div class="line">  <span class="keyword">return</span>(predictions)</div><div class="line">}</div></pre></td></tr></table></figure>

<hr>
<h3 id="正确率的计算">正确率的计算</h3>
<p>将 knn 函数返回的 Label 预测向量利用 <code>transform</code> 函数添加到原数据框中</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">data &lt;- transform(data, kNNPredictions = knn(data))</div></pre></td></tr></table></figure>

<p>预测错误了多少条？</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sum(with(data, Label != kNNPredictions))</div></pre></td></tr></table></figure>



<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="preprocessor">## [1] 7</span></div></pre></td></tr></table></figure>

<p>knn 算法的正确率多少？</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sum(with(data, Label == kNNPredictions))/nrow(data)</div></pre></td></tr></table></figure>



<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="preprocessor">## [1] 0.93</span></div></pre></td></tr></table></figure>

<p>可以看到，我们的正确率还是不错的，不过仍然有一些点是预测错误的。具体是哪些点呢？</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">with(data,which(Label != kNNPredictions))</div></pre></td></tr></table></figure>



<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="preprocessor">## [1]  5 11 18 20 84 93 95</span></div></pre></td></tr></table></figure>

<p>我们可以将这些点在图中标注出来，用三角形来表示预测错误的点</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">index &lt;- with(data,which(Label != kNNPredictions))</div><div class="line">data$differ &lt;- <span class="string">"right"</span></div><div class="line">data$differ[index] &lt;- <span class="string">"wrong"</span></div><div class="line">visual + geom_point(aes(shape = factor(data$differ))) + scale_shape_manual(values = c(<span class="number">21</span>,<span class="number">24</span>)) + scale_colour_brewer(palette = <span class="string">"Set1"</span>)</div></pre></td></tr></table></figure>

<p><img src="/img/knn/wrong.png" alt="wrong"> </p>
<p>可以看到，预测错误的点都是“深入敌后”的样本点。</p>
<hr>
<h2 id="使用_class_包实现_knn_算法">使用 class 包实现 knn 算法</h2>
<p>class 包中有三个和 knn 算法相关的函数，分别是</p>
<ul>
<li>knn k-Nearest Neighbour Classification</li>
<li>knn1 1-nearest neighbour classification</li>
<li>knn.cv k-Nearest Neighbour Cross-Validatory Classification</li>
</ul>
<p>关于三个函数的参数说明和使用方法，请见<a href="http://ripeconan.com/2014/11/10/class_knn/" target="_blank" rel="external">class 包中的 knn 系列函数</a>一文</p>
<hr>
<h3 id="数据准备">数据准备</h3>
<p>我们仍然采用前面的数据集 <code>data</code></p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">head(data)</div></pre></td></tr></table></figure>



<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="preprocessor">##          X        Y Label kNNPredictions differ</span></div><div class="line"><span class="preprocessor">## 1 2.373546 5.398106     0              0  right</span></div><div class="line"><span class="preprocessor">## 2 3.183643 4.387974     0              0  right</span></div><div class="line"><span class="preprocessor">## 3 2.164371 5.341120     0              0  right</span></div><div class="line"><span class="preprocessor">## 4 4.595281 3.870637     0              0  right</span></div><div class="line"><span class="preprocessor">## 5 3.329508 6.433024     0              1  wrong</span></div><div class="line"><span class="preprocessor">## 6 2.179532 6.980400     0              0  right</span></div></pre></td></tr></table></figure>

<p>我们只保留原始数据，因此去掉 <code>data</code> 数据集的后两列</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">data &lt;- data[c(<span class="string">"X"</span>,<span class="string">"Y"</span>,<span class="string">"Label"</span>)]</div><div class="line">head(data)</div></pre></td></tr></table></figure>



<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="preprocessor">##          X        Y Label</span></div><div class="line"><span class="preprocessor">## 1 2.373546 5.398106     0</span></div><div class="line"><span class="preprocessor">## 2 3.183643 4.387974     0</span></div><div class="line"><span class="preprocessor">## 3 2.164371 5.341120     0</span></div><div class="line"><span class="preprocessor">## 4 4.595281 3.870637     0</span></div><div class="line"><span class="preprocessor">## 5 3.329508 6.433024     0</span></div><div class="line"><span class="preprocessor">## 6 2.179532 6.980400     0</span></div></pre></td></tr></table></figure>

<p>由于我们在上文中已经定义了 knn 函数，这与即将要加载的 <strong>class</strong> 包中 knn 函数同名，因此我们需要先删除之前定义的 knn 函数，以免引起混淆</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">rm(knn)</div><div class="line"><span class="keyword">library</span>(class)</div></pre></td></tr></table></figure>

<p>接下来，我们为 class 包中的 knn 函数和 knn1 函数准备训练集和测试集</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">trainset &lt;- data[c(<span class="number">1</span>:<span class="number">25</span>,<span class="number">51</span>:<span class="number">75</span>),<span class="number">1</span>:<span class="number">2</span>]</div><div class="line">testset &lt;- data[c(<span class="number">26</span>:<span class="number">50</span>,<span class="number">76</span>:<span class="number">100</span>),<span class="number">1</span>:<span class="number">2</span>]</div><div class="line">trainlabel &lt;- data[c(<span class="number">1</span>:<span class="number">25</span>,<span class="number">51</span>:<span class="number">75</span>),<span class="number">3</span>]</div></pre></td></tr></table></figure>

<hr>
<h3 id="使用_class::knn_和_class::knn1_进行分类">使用 class::knn 和 class::knn1 进行分类</h3>
<p>针对上述测试集，我们对其分类进行预测</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">pred.knn &lt;- knn(train = trainset,test = testset,cl = trainlabel, k = <span class="number">3</span>)</div><div class="line">pred.knn1 &lt;- knn1(train = trainset,test = testset,cl = trainlabel)</div></pre></td></tr></table></figure>

<p>我们来看一下使用 knn 和 knn1 的各自分类效果如何</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">testlabel &lt;- data[c(<span class="number">26</span>:<span class="number">50</span>,<span class="number">76</span>:<span class="number">100</span>),<span class="number">3</span>]</div><div class="line">accuracy.knn &lt;- sum(pred.knn == testlabel)/length(testlabel)</div><div class="line">accuracy.knn1 &lt;- sum(pred.knn1 == testlabel)/length(testlabel)</div><div class="line">accuracy.knn</div></pre></td></tr></table></figure>



<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="preprocessor">## [1] 0.86</span></div></pre></td></tr></table></figure>



<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">accuracy.knn1</div></pre></td></tr></table></figure>



<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="preprocessor">## [1] 0.86</span></div></pre></td></tr></table></figure>

<hr>
<h3 id="使用_class::knn-cv_进行分类">使用 class::knn.cv 进行分类</h3>
<p>对于 knn.cv 函数，我们把所有的数据都放到训练集中去</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">trainset &lt;- data[<span class="number">1</span>:<span class="number">100</span>,<span class="number">1</span>:<span class="number">2</span>]</div><div class="line">trainlabel &lt;- data[<span class="number">1</span>:<span class="number">100</span>,<span class="number">3</span>]</div></pre></td></tr></table></figure>

<p>使用 knn.cv 进行预测，并计算其准确率</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sum(knn.cv(trainset,trainlabel,k = <span class="number">3</span>) == trainlabel)/length(trainlabel)</div></pre></td></tr></table></figure>



<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="preprocessor">## [1] 0.91</span></div></pre></td></tr></table></figure>

<p>可以看到，同样使用 k = 3 的参数，knn.cv 比 knn 的表现要好一些</p>
<hr>
<h2 id="使用_caret_包实现_knn_算法">使用 caret 包实现 knn 算法</h2>
<p>仍然使用上述数据集 <code>data</code>, 我们使用 caret 包中的 createDataPartition 函数将数据集划分为训练集和测试集，然后使用 caret 包中的 knn3 函数和 predict.knn3 函数进行分类预测，并最终计算分类准确率</p>
<hr>
<h3 id="使用_createDataPartition_函数划分训练集和测试集">使用 createDataPartition 函数划分训练集和测试集</h3>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">library</span>(caret)</div></pre></td></tr></table></figure>



<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">## Loading <span class="keyword">required</span> <span class="keyword">package</span>: lattice</div></pre></td></tr></table></figure>



<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">set.seed(<span class="number">2014</span>)</div><div class="line">createDataPartition(y = data$Label, times = <span class="number">1</span>, p = <span class="number">0.5</span>)</div></pre></td></tr></table></figure>



<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="preprocessor">## $Resample1</span></div><div class="line"><span class="preprocessor">##  [1]   2   3   4   5   7   9  15  17  20  22  23  24  26  31  32  35  36</span></div><div class="line"><span class="preprocessor">## [18]  38  40  41  43  44  46  47  50  52  57  58  61  62  65  67  69  70</span></div><div class="line"><span class="preprocessor">## [35]  72  74  76  77  78  79  83  87  88  89  90  93  96  98  99 100</span></div></pre></td></tr></table></figure>

<p>在 createDataPartition 函数中，主要有三个参数(其他参数请查阅帮助文档)</p>
<ul>
<li>y 真实分类数据</li>
<li>times 从 y 中创建的 partition 的个数，除非重复实验，否则需要一个就行</li>
<li>p 训练集占数据集的比重</li>
</ul>
<p>使用 createDataPartition 的好处在于，它能将低熵数据集随机抽取出我们需要的训练集来。比如我们的数据集共有 100 个样本点，前 50 个是一类，后 50 个是一类。我们为了让训练集里两类样本都各有一些，必然希望从前 50 个样本点随机抽取一定比例，后 50 个里也随机抽取相应比例的样本点来组成训练集。这个手动过程因为涉及到人的主观意识，从而不能保证完全随机化。而 createDataPartition 会自动从 y 的各个 level 随机取出等比例的数据来，组成训练集，给我们省了很多事。</p>
<p>createDataPartition 函数默认返回的是列表，我们将其中的训练集对应坐标提取出来，并用其去划分训练集</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">set.seed(<span class="number">2014</span>)</div><div class="line">index.caret &lt;- createDataPartition(y = data$Label, times = <span class="number">1</span>, p = <span class="number">0.5</span>)[[<span class="number">1</span>]]</div></pre></td></tr></table></figure>

<hr>
<h3 id="使用_knn3_函数进行分类预测">使用 knn3 函数进行分类预测</h3>
<p>这一步要使用 caret 包的两个函数</p>
<ul>
<li>knn3 用来建模，有好几种实现形式，我们选用公式形式，返回的是一个 knn3 类型</li>
<li>predict.knn3，也可以写成 predict，针对 knn3 类型的模型，提供测试数据集，可以预测</li>
</ul>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">data.fit &lt;- knn3(factor(Label) ~ ., data, index.caret, k = <span class="number">3</span>)</div><div class="line">predict.caret &lt;- predict(data.fit,newdata = data[-index.caret,<span class="number">1</span>:<span class="number">2</span>],type = <span class="string">"class"</span>)</div><div class="line">predict.caret</div></pre></td></tr></table></figure>



<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="preprocessor">##  [1] 0 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1</span></div><div class="line"><span class="preprocessor">## [36] 1 1 1 1 1 1 1 0 1 0 1 1 1 0 1</span></div><div class="line"><span class="preprocessor">## Levels: 0 1</span></div></pre></td></tr></table></figure>

<p>我们再把预测的结果和测试集的真实分类作比较，计算预测准确率</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sum(predict.caret == data[-index.caret,<span class="number">3</span>])/length(index.caret)</div></pre></td></tr></table></figure>



<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="preprocessor">## [1] 0.88</span></div></pre></td></tr></table></figure>

<p>准确率也不错。</p>
<hr>
<p>综上所述，在本篇文章中的数据集下，用自己编写代码的方式、class包、caret包都能取得较好的预测准确率。</p>
  
	</div>
		<footer class="article-footer clearfix">

  <div class="article-tags">
  
  <span></span> <a href="/tags/data-mining/">data mining</a><a href="/tags/algorithms/">algorithms</a>
  </div>


<div class="article-categories">
  <span></span>
  <a class="article-category-link" href="/categories/Machine-Learning/">Machine Learning</a>
</div>



<div class="article-share" id="share">

  <div data-url="http://ripeconan.com/2014/11/11/knn/" data-title="knn 算法简介和三种实现方式 | ripeconan" data-tsina="null" class="share clearfix">
  </div>

</div>
</footer>   	       
	</article>
	
<nav class="article-nav clearfix">
 
 <div class="prev" >
 <a href="/2014/11/13/breast_cancer/" title="基于 knn 方法分析乳腺癌数据">
  <strong>PREVIOUS:</strong><br/>
  <span>
  基于 knn 方法分析乳腺癌数据</span>
</a>
</div>


<div class="next">
<a href="/2014/11/10/class_knn/"  title="class 包中的 knn 系列函数">
 <strong>NEXT:</strong><br/> 
 <span>class 包中的 knn 系列函数
</span>
</a>
</div>

</nav>

	
<section class="comment">
	<div class="ds-thread"></div>
</section>

</div>  
      <div class="openaside"><a class="navbutton" href="#" title="Show Sidebar"></a></div>

  <div id="toc" class="toc-aside">
  <strong class="toc-title">Contents</strong>
  <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#knn_算法介绍"><span class="toc-number">1.</span> <span class="toc-text">knn 算法介绍</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#使用自己编写的代码实现_knn_算法"><span class="toc-number">2.</span> <span class="toc-text">使用自己编写的代码实现 knn 算法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#数据集介绍"><span class="toc-number">2.1.</span> <span class="toc-text">数据集介绍</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#距离矩阵定义"><span class="toc-number">2.2.</span> <span class="toc-text">距离矩阵定义</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#knn_函数的定义"><span class="toc-number">2.3.</span> <span class="toc-text">knn 函数的定义</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#正确率的计算"><span class="toc-number">2.4.</span> <span class="toc-text">正确率的计算</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#使用_class_包实现_knn_算法"><span class="toc-number">3.</span> <span class="toc-text">使用 class 包实现 knn 算法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#数据准备"><span class="toc-number">3.1.</span> <span class="toc-text">数据准备</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#使用_class::knn_和_class::knn1_进行分类"><span class="toc-number">3.2.</span> <span class="toc-text">使用 class::knn 和 class::knn1 进行分类</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#使用_class::knn-cv_进行分类"><span class="toc-number">3.3.</span> <span class="toc-text">使用 class::knn.cv 进行分类</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#使用_caret_包实现_knn_算法"><span class="toc-number">4.</span> <span class="toc-text">使用 caret 包实现 knn 算法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#使用_createDataPartition_函数划分训练集和测试集"><span class="toc-number">4.1.</span> <span class="toc-text">使用 createDataPartition 函数划分训练集和测试集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#使用_knn3_函数进行分类预测"><span class="toc-number">4.2.</span> <span class="toc-text">使用 knn3 函数进行分类预测</span></a></li></ol></li></ol>
  </div>

<div id="asidepart">
<div class="closeaside"><a class="closebutton" href="#" title="Hide Sidebar"></a></div>
<aside class="clearfix">
<div id="authorInfo">
	
		<div class="author-logo"></div>		
	
	<div class="social-list" class="clearfix">
		
		
		
		
		
	</div>
</div>

  
<div class="categorieslist">
	<p class="asidetitle">Categories</p>
		<ul>
		
			<li><a href="/categories/Learning/" title="Learning">Learning<sup>1</sup></a></li>
		
			<li><a href="/categories/Machine-Learning/" title="Machine Learning">Machine Learning<sup>3</sup></a></li>
		
			<li><a href="/categories/R/" title="R">R<sup>1</sup></a></li>
		
			<li><a href="/categories/Visualization/" title="Visualization">Visualization<sup>1</sup></a></li>
		
		</ul>
</div>


  
<div class="tagslist">
	<p class="asidetitle">Tags</p>
		<ul class="clearfix">
		
			<li><a href="/tags/Excel/" title="Excel">Excel<sup>1</sup></a></li>
		
			<li><a href="/tags/R-package/" title="R package">R package<sup>1</sup></a></li>
		
			<li><a href="/tags/R函数/" title="R函数">R函数<sup>2</sup></a></li>
		
			<li><a href="/tags/To-Do/" title="To Do">To Do<sup>1</sup></a></li>
		
			<li><a href="/tags/algorithms/" title="algorithms">algorithms<sup>1</sup></a></li>
		
			<li><a href="/tags/data-mining/" title="data mining">data mining<sup>2</sup></a></li>
		
			<li><a href="/tags/可视化/" title="可视化">可视化<sup>1</sup></a></li>
		
		</ul>
</div>


  <div class="linkslist">
  <p class="asidetitle">Links</p>
    <ul>
      <li><a href="http://zjdian.com" target="_blank" title="zjdian">中继点</a></li>
      <li><a href="http://blog.fens.me" target="_blank" title="fens">粉丝日志</a></li>
      <li><a href="http://gengbiao.me/" target="_blank" title="coney">coney's Blog</a></li>
      <li><a href="http://tim4chan.com/" target="_blank" title="fens">Tim Chan's Blog</a></li>
    </ul>
</div>



  <div class="rsspart">
	<a href="atom.xml" target="_blank" title="rss">RSS</a>
</div>

  
  <div class="tagcloudlist">
    <p class="asidetitle">Tag Cloud</p>
    <div class="tagcloudlist clearfix">
       <a href="/tags/Excel/" style="font-size: 10.00px;">Excel</a><a href="/tags/R-package/" style="font-size: 10.00px;">R package</a><a href="/tags/R函数/" style="font-size: 20.00px;">R函数</a><a href="/tags/To-Do/" style="font-size: 10.00px;">To Do</a><a href="/tags/algorithms/" style="font-size: 10.00px;">algorithms</a><a href="/tags/data-mining/" style="font-size: 20.00px;">data mining</a><a href="/tags/可视化/" style="font-size: 10.00px;">可视化</a>
    </div>
  </div>


</aside>
</div>
    </div>
    <footer><div id="footer" >
    
            <p class="copyright"> © 2014 
		
		<a href="http://ripeconan.com" target="_blank" title="ripeconan">ripeconan</a>
		
            && Powered by <a href="http://hexo.io" target="_blank" title="hexo">hexo</a> && Theme by <a href="http://gengbiao.me" target="_blank" title="coney">coney</a>
            </div>
</footer>
    <script src="/js/jquery-2.1.0.min.js"></script>
<script type="text/javascript">
$(document).ready(function(){ 
  $('.navbar').click(function(){
    $('header nav').toggleClass('shownav');
  });
  var myWidth = 0;
  function getSize(){
    if( typeof( window.innerWidth ) == 'number' ) {
      myWidth = window.innerWidth;
    } else if( document.documentElement && document.documentElement.clientWidth) {
      myWidth = document.documentElement.clientWidth;
    };
  };
  var m = $('#main'),
      a = $('#asidepart'),
      c = $('.closeaside'),
      o = $('.openaside');
  $(window).resize(function(){
    getSize(); 
    if (myWidth >= 1024) {
      $('header nav').removeClass('shownav');
    }else
    {
      m.removeClass('moveMain');
      a.css('display', 'block').removeClass('fadeOut');
      o.css('display', 'none');
      
      $('#toc.toc-aside').css('display', 'none');
        
    }
  });
  c.click(function(){
    a.addClass('fadeOut').css('display', 'none');
    o.css('display', 'block').addClass('fadeIn');
    m.addClass('moveMain');
  });
  o.click(function(){
    o.css('display', 'none').removeClass('beforeFadeIn');
    a.css('display', 'block').removeClass('fadeOut').addClass('fadeIn');      
    m.removeClass('moveMain');
  });
  $(window).scroll(function(){
    o.css("top",Math.max(80,260-$(this).scrollTop()));
  });
});
</script>

<script type="text/javascript">
$(document).ready(function(){ 
  var ai = $('.article-content>iframe'),
      ae = $('.article-content>embed'),
      t  = $('#toc'),
      h  = $('article h2')
      ah = $('article h2'),
      ta = $('#toc.toc-aside'),
      o  = $('.openaside'),
      c  = $('.closeaside');
  if(ai.length>0){
    ai.wrap('<div class="video-container" />');
  };
  if(ae.length>0){
   ae.wrap('<div class="video-container" />');
  };
  if(ah.length==0){
    t.css('display','none');
  }else{
    c.click(function(){
      ta.css('display', 'block').addClass('fadeIn');
    });
    o.click(function(){
      ta.css('display', 'none');
    });
    $(window).scroll(function(){
      ta.css("top",Math.max(140,320-$(this).scrollTop()));
    });
  };
});
</script>


<script type="text/javascript">
$(document).ready(function(){ 
  var $this = $('.share'),
      url = $this.attr('data-url'),
      encodedUrl = encodeURIComponent(url),
      title = $this.attr('data-title'),
      tsina = $this.attr('data-tsina');
  var html = [
  '<a href="#" class="overlay" id="qrcode"></a>',
  '<div class="qrcode clearfix"><span>扫描二维码分享到微信朋友圈</span><a class="qrclose" href="#share"></a><strong>Loading...Please wait</strong><img id="qrcode-pic" data-src="http://s.jiathis.com/qrcode.php?url=' + encodedUrl + '"/></div>',
  '<a href="#textlogo" class="article-back-to-top" title="Top"></a>',
  '<a href="https://www.facebook.com/sharer.php?u=' + encodedUrl + '" class="article-share-facebook" target="_blank" title="Facebook"></a>',
  '<a href="#qrcode" class="article-share-qrcode" title="QRcode"></a>',
  '<a href="https://twitter.com/intent/tweet?url=' + encodedUrl + '" class="article-share-twitter" target="_blank" title="Twitter"></a>',
  '<a href="http://service.weibo.com/share/share.php?title='+title+'&url='+encodedUrl +'&ralateUid='+ tsina +'&searchPic=true&style=number' +'" class="article-share-weibo" target="_blank" title="Weibo"></a>',
  '<span title="Share to"></span>'
  ].join('');
  $this.append(html);
  $('.article-share-qrcode').click(function(){
    var imgSrc = $('#qrcode-pic').attr('data-src');
    $('#qrcode-pic').attr('src', imgSrc);
    $('#qrcode-pic').load(function(){
        $('.qrcode strong').text(' ');
    });
  });
});     
</script>


<script type="text/javascript">
  var duoshuoQuery = {short_name:"null"};
  (function() {
    var ds = document.createElement('script');
    ds.type = 'text/javascript';ds.async = true;
    ds.src = '//static.duoshuo.com/embed.js';
    ds.charset = 'UTF-8';
    (document.getElementsByTagName('head')[0] 
    || document.getElementsByTagName('body')[0]).appendChild(ds);
  })();
</script> 





<script>
    
        var _bdImg = '4';
    
    window._bd_share_config={
        "common":{
            "bdSnsKey":{

            },
            "bdText":"",
            "bdMini":"2",
            "bdMiniList":[
                "qzone",
                "tsina",
                "weixin",
                "renren",
                "tqq",
                "tieba",
                "douban",
                "sqq",
                "diandian",
                "huaban",
                "youdao",
                "mail",
                "ty",
                "fbook",
                "twi",
                "linkedin",
                "copy",
                "print"
            ],
            "bdPic":"",
            "bdStyle":"0",
            "bdSize":"16"
        },
        "slide":{
            "type":"slide",
            "bdImg":_bdImg,
            "bdPos":"right",
            "bdTop":"350"
        },
        "image":{
            "viewList":[
                "weixin",
                "qzone",
                "tsina",
                "renren",
                "douban",
                "tqq"
            ],
            "viewText":"分享：",
            "viewSize":"16"
        },
        "selectShare":{
            "bdContainerClass":null,
            "bdSelectMiniList":[
                "weixin",
                "qzone",
                "tsina",
                "renren",
                "douban",
                "tqq"
            ]
        }
    };
    with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];
</script>




<script type="text/javascript">
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');
ga('create', 'null', 'null');  
ga('send', 'pageview');
</script>


  </body>
</html>

