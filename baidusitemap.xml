<?xml version="1.0" encoding="UTF-8"?>
<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">

    
  <url>
    <loc>http://ripeconan.com/2014/11/13/breast_cancer/</loc>
    <lastmod>2014-11-13T07:31:39.000Z</lastmod>
    <data>
        <display>
        <title>基于 knn 方法分析乳腺癌数据</title>
        <pubTime>2014-11-12T16:00:00.000Z</pubTime>
        
        <tag>data mining </tag>
         
         <content><![CDATA[<h2 id="数据来源">数据来源</h2>
<p>数据：来自于 UCI 常用数据，也是机器学习的经典数据<br><a href="http://mlr.cs.umass.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data" target="_blank" rel="external">http://mlr.cs.umass.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data</a></p>
<hr>
<p>数据说明：对数据变量的解释和其他解说请看<br><a href="http://mlr.cs.umass.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.names" target="_blank" rel="external">http://mlr.cs.umass.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.names</a></p>
<hr>
<h2 id="数据了解">数据了解</h2>
<p>根据数据说明，我们可以得知，该数据集共569个样本，32个特征，需要预测的是第二个特征 <strong>diagnosis: B = benign, M = malignant</strong>，即乳腺癌是良性还是恶性的  </p>
<hr>
<p>其中第一个特征是用户id，从第3个到第32个特征都是十个特征的不同度量，其中十个特征是指</p>
<pre><code><span class="operator">a</span>) radius (mean <span class="operator">of</span> distances <span class="built_in">from</span> center <span class="built_in">to</span> points <span class="command"><span class="keyword">on</span> <span class="title">the</span> <span class="title">perimeter</span>)</span>
b) texture (standard deviation <span class="operator">of</span> gray-scale values)
c) perimeter
d) area
e) smoothness (<span class="built_in">local</span> variation <span class="operator">in</span> radius lengths)
f) compactness (perimeter^<span class="number">2</span> / area - <span class="number">1.0</span>)
g) concavity (severity <span class="operator">of</span> concave portions <span class="operator">of</span> <span class="operator">the</span> contour)
h) concave points (<span class="built_in">number</span> <span class="operator">of</span> concave portions <span class="operator">of</span> <span class="operator">the</span> contour)
i) symmetry 
j) fractal dimension (<span class="string">"coastline approximation"</span> - <span class="number">1</span>)
</code></pre><p>3-12是这十个特征的 <strong>mean</strong>, 13-22是它们的 <strong>standard error</strong>, 23-32是 <strong>mean of the three largest values</strong></p>
<hr>
<p>之前最好的预测：</p>
<pre><code>best predictive accuracy obtained <span class="keyword">using</span> <span class="constant">one</span> separating plane
<span class="operator">in</span> <span class="operator">the</span> <span class="number">3</span>-D <span class="constant">space</span> <span class="operator">of</span> Worst Area, Worst Smoothness <span class="operator">and</span>
Mean Texture.  Estimated accuracy <span class="number">97.5</span>% <span class="keyword">using</span> repeated
<span class="number">10</span>-fold crossvalidations.  Classifier has correctly
diagnosed <span class="number">176</span> consecutive <span class="built_in">new</span> patients <span class="keyword">as</span> <span class="operator">of</span> November
<span class="number">1995.</span> 
</code></pre><p>针对以上数据集，我们将分别使用 <strong>class</strong> 包和 <strong>caret</strong> 包来进行 <strong>knn</strong> 算法的实现</p>
<hr>
<h2 id="数据处理">数据处理</h2>
<hr>
<h3 id="数据读取">数据读取</h3>
<p>首先，从网络读取数据，并查看一下，与数据说明的描述一致</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">wdbc &lt;- read.table(<span class="string">"http://mlr.cs.umass.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data"</span>,sep = <span class="string">","</span>)</div><div class="line">dim(wdbc)</div></pre></td></tr></table></figure>



<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="preprocessor">## [1] 569  32</span></div></pre></td></tr></table></figure>

<hr>
<h3 id="数据预处理">数据预处理</h3>
<p>我们需要给变量命名</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">wdbc.names=c(<span class="string">"Radius"</span>,<span class="string">"Texture"</span>,<span class="string">"Perimeter"</span>,<span class="string">"Area"</span>,<span class="string">"Smoothness"</span>,<span class="string">"Compactness"</span>,<span class="string">"Concavity"</span>,<span class="string">"Concave points"</span>,<span class="string">"Symmetry"</span>,<span class="string">"Fractal dimension"</span>)</div><div class="line">wdbc.names=c(paste(wdbc.names,<span class="string">"_mean"</span>,sep=<span class="string">""</span>),paste(wdbc.names,<span class="string">"_se"</span>,sep=<span class="string">""</span>),paste(wdbc.names,<span class="string">"_worst"</span>,sep=<span class="string">""</span>))</div><div class="line">names(wdbc)=c(<span class="string">"id"</span>,<span class="string">"diagnosis"</span>,wdbc.names)</div></pre></td></tr></table></figure>

<p>虽然可以使用所有的 30 个特征来预测乳腺癌的性质， 但是我们只验证在数据说明中提到的最好预测情形， 即使用 <em>Worst Area</em>, <em>Worst Smoothness</em> 和 <em>Mean Texture</em> 三个特征来进行预测。 为此，我们提取出相关数据</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">wdbc.short &lt;- wdbc[c(<span class="string">"diagnosis"</span>,<span class="string">"Area_worst"</span>,<span class="string">"Smoothness_worst"</span>,<span class="string">"Texture_mean"</span>)]</div><div class="line">summary(wdbc.short)</div></pre></td></tr></table></figure>



<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">##  <span class="tag">diagnosis</span>   <span class="tag">Area_worst</span>     <span class="tag">Smoothness_worst</span>   <span class="tag">Texture_mean</span>  </div><div class="line">##  <span class="tag">B</span><span class="pseudo">:357</span>     <span class="tag">Min</span>.   : 185<span class="class">.2</span>   <span class="tag">Min</span>.   <span class="pseudo">:0</span><span class="class">.07117</span>   <span class="tag">Min</span>.   : 9<span class="class">.71</span>  </div><div class="line">##  <span class="tag">M</span><span class="pseudo">:212</span>     1<span class="tag">st</span> <span class="tag">Qu</span>.: 515<span class="class">.3</span>   1<span class="tag">st</span> <span class="tag">Qu</span>.<span class="pseudo">:0</span><span class="class">.11660</span>   1<span class="tag">st</span> <span class="tag">Qu</span>.<span class="pseudo">:16</span><span class="class">.17</span>  </div><div class="line">##            <span class="tag">Median</span> : 686<span class="class">.5</span>   <span class="tag">Median</span> <span class="pseudo">:0</span><span class="class">.13130</span>   <span class="tag">Median</span> <span class="pseudo">:18</span><span class="class">.84</span>  </div><div class="line">##            <span class="tag">Mean</span>   : 880<span class="class">.6</span>   <span class="tag">Mean</span>   <span class="pseudo">:0</span><span class="class">.13237</span>   <span class="tag">Mean</span>   <span class="pseudo">:19</span><span class="class">.29</span>  </div><div class="line">##            3<span class="tag">rd</span> <span class="tag">Qu</span>.<span class="pseudo">:1084</span><span class="class">.0</span>   3<span class="tag">rd</span> <span class="tag">Qu</span>.<span class="pseudo">:0</span><span class="class">.14600</span>   3<span class="tag">rd</span> <span class="tag">Qu</span>.<span class="pseudo">:21</span><span class="class">.80</span>  </div><div class="line">##            <span class="tag">Max</span>.   <span class="pseudo">:4254</span><span class="class">.0</span>   <span class="tag">Max</span>.   <span class="pseudo">:0</span><span class="class">.22260</span>   <span class="tag">Max</span>.   <span class="pseudo">:39</span><span class="class">.28</span></div></pre></td></tr></table></figure>

<p>由于这三个特征的数据范围相差太大，为此，我们使用 <strong>scale</strong> 函数将其标准化</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">wdbc.short[<span class="number">2</span>:<span class="number">4</span>] &lt;- scale(wdbc.short[<span class="number">2</span>:<span class="number">4</span>])</div><div class="line">summary(wdbc.short)</div></pre></td></tr></table></figure>



<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="preprocessor">##  diagnosis   Area_worst      Smoothness_worst   Texture_mean    </span></div><div class="line"><span class="preprocessor">##  B:357     Min.   :-1.2213   Min.   :-2.6803   Min.   :-2.2273  </span></div><div class="line"><span class="preprocessor">##  M:212     1st Qu.:-0.6416   1st Qu.:-0.6906   1st Qu.:-0.7253  </span></div><div class="line"><span class="preprocessor">##            Median :-0.3409   Median :-0.0468   Median :-0.1045  </span></div><div class="line"><span class="preprocessor">##            Mean   : 0.0000   Mean   : 0.0000   Mean   : 0.0000  </span></div><div class="line"><span class="preprocessor">##            3rd Qu.: 0.3573   3rd Qu.: 0.5970   3rd Qu.: 0.5837  </span></div><div class="line"><span class="preprocessor">##            Max.   : 5.9250   Max.   : 3.9519   Max.   : 4.6478</span></div></pre></td></tr></table></figure>

<hr>
<h3 id="使用_caret_包进行_knn_预测">使用 caret 包进行 knn 预测</h3>
<p>接下来，我们先使用 <strong>caret</strong> 包，对该数据集进行预测。 首先把数据集分为训练集和测试集，各取一半样本点。</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">library</span>(caret)</div><div class="line">index &lt;- createDataPartition(wdbc.short$diagnosis)</div><div class="line">caret.fit &lt;- knn3(diagnosis ~ ., wdbc.short, index[[<span class="number">1</span>]], k = <span class="number">5</span>)</div><div class="line">predict.caret &lt;- predict(caret.fit,newdata = wdbc.short[-index[[<span class="number">1</span>]],<span class="number">2</span>:<span class="number">4</span>], type = <span class="string">"class"</span>)</div></pre></td></tr></table></figure>

<p>我们来看一下使用 <strong>caret</strong> 包对乳腺癌测试集预测的准确性</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sum(wdbc.short[-index[[<span class="number">1</span>]],<span class="number">1</span>] == predict.caret)/length(index[[<span class="number">1</span>]])</div></pre></td></tr></table></figure>



<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="preprocessor">## [1] 0.9403509</span></div></pre></td></tr></table></figure>

<p>准确率还是不错的。</p>
<hr>
<h3 id="10-fold_crossvalidations">10-fold crossvalidations</h3>
<p>我们看到在数据说明里，使用 <strong>repeated 10-fold crossvalidations</strong> 可以达到 <strong>97.5%</strong> 的准确率。 我们使用 <strong>cvTools</strong> 包中的 <strong>cvFolds</strong> 函数产生用于进行 10折交叉验证的下标，但是前提条件是被分组的变量需是排过序的，因此我们使用 <strong>reshape</strong> 包中的 <strong>sort_df</strong> 来对数据集进行排序</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">library</span>(cvTools)</div><div class="line"><span class="keyword">library</span>(reshape)</div><div class="line">wdbc.short &lt;- sort_df(wdbc.short, <span class="string">"diagnosis"</span>)</div></pre></td></tr></table></figure>

<hr>
<p>然后，分别对良性和恶性两类样本分别分为十组。由于 <strong>cvFolds</strong> 函数返回的是一个特有类的对象，我们要从中抽取出想要的部分，并使用 <strong>rbind</strong> 和 <strong>cbind</strong> 合并为下标矩阵 <code>index.matrix</code></p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">set.seed(<span class="number">2014</span>)</div><div class="line">Benign.folds &lt;- cvFolds(n = as.numeric(table(wdbc.short$diagnosis)[<span class="string">"B"</span>]), K = <span class="number">10</span>, type = <span class="string">"random"</span>)</div><div class="line">set.seed(<span class="number">2014</span>)</div><div class="line">Malignant.folds &lt;- cvFolds(n = as.numeric(table(wdbc.short$diagnosis)[<span class="string">"M"</span>]), K = <span class="number">10</span>, type = <span class="string">"random"</span>)</div><div class="line">index.matrix &lt;- rbind(cbind(Benign.folds$which,as.vector(Benign.folds$subsets)),cbind(Malignant.folds$which,as.vector(Malignant.folds$subsets)+as.numeric(table(wdbc.short$diagnosis)[<span class="string">"B"</span>])))</div></pre></td></tr></table></figure>

<hr>
<p>由于一共有 569 个样本，无法均匀分成 10 组，所以我们需要用一个因子 <strong>index.folds</strong> 来存放各个组对应样本的下标。对于每个组，我们使用另外 9 组建模，并对该组进行验证，此即交叉验证。 <code>prediction.right</code> 向量用来存放每组验证的正确率.</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">index.folds &lt;- list()</div><div class="line">prediction.right &lt;- vector()</div><div class="line"><span class="keyword">for</span>(i <span class="keyword">in</span> <span class="number">1</span>:<span class="number">10</span>){</div><div class="line">  index.folds[[i]] &lt;- index.matrix[index.matrix[,<span class="number">1</span>] == i,<span class="number">2</span>]</div><div class="line">}</div><div class="line"><span class="keyword">for</span>(i <span class="keyword">in</span> <span class="number">1</span>:<span class="number">10</span>){</div><div class="line">  current.model &lt;- knn3(diagnosis ~ ., data = wdbc.short, subset = -index.folds[[i]])</div><div class="line">  prediction &lt;- predict(current.model, newdata = wdbc.short[index.folds[[i]],<span class="number">2</span>:<span class="number">4</span>], type = <span class="string">"class"</span>)</div><div class="line">  prediction.right[i] &lt;- sum(prediction == wdbc.short[index.folds[[i]],<span class="string">"diagnosis"</span>])</div><div class="line">}</div><div class="line">sum(prediction.right)/nrow(wdbc.short)</div></pre></td></tr></table></figure>



<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="preprocessor">## [1] 0.9507909</span></div></pre></td></tr></table></figure>

<hr>
<p>最后我们看到，使用 <strong>10-fold crossvalidations</strong> 的正确率为 0.9507909, 虽然优于没有使用交叉验证的情况，但是并没有达到预想中的 97.5%, 这可能和我们并没有使用 <strong>repeat</strong> 方法有关。</p>
]]></content>
         
         
           
             
              <breadCrumb title="Machine Learning" url="http://ripeconan.com/categories/Machine-Learning/"/>
          
        </display>
    </data>
    </url>

    
  <url>
    <loc>http://ripeconan.com/2014/11/11/knn/</loc>
    <lastmod>2014-11-11T04:12:28.000Z</lastmod>
    <data>
        <display>
        <title>knn 算法简介和三种实现方式</title>
        <pubTime>2014-11-10T16:00:00.000Z</pubTime>
        
        <tag>data mining </tag>
         
        <tag>algorithms </tag>
         
         <content><![CDATA[<h2 id="knn_算法介绍">knn 算法介绍</h2>
<p>knn 是 <strong>k-nearest neighbour</strong> 的缩写，又名 k 邻近。knn 算法的核心思想是“近朱者赤，近墨者黑”。拿一个电子商务系统举例来说，该系统在为你做商品推荐的时候，需要根据和你具有相似特征的人群的购买记录来为你推荐。它的理由就是：如果他们和你是类似的人的话，那么他们想要购买的东西，想必你也是需要买的，所以推荐给你。那么如何定义和你具有相似特征的人呢？这些特征可能是性别、年龄、地域、浏览记录、购买记录、收藏记录等各种数据。对于这些不同类别的数据，无论是分类数据，还是数值型数据，我们都有若干种方法来定义两个样本点之间的距离，比如最常见的<strong>欧氏距离，曼哈顿距离</strong>等。通过利用合适的方法来度量距离，我们可以得到谁和你是类似的人。那么他们喜欢的东西，系统经过排序，将会将结果展现给你。</p>
<p>除了使用哪种距离以外，我们还需要关注的是，需要综合你周围的几个人的意见来为你做推荐？是需要少数几个人就够，还是需要很多人的综合意见？这就是 knn 算法中的参数 k 的含义。通常情况下，选择不同的 k 会使得我们的算法的表现有好有坏，我们需要对 k 经过多种尝试，来决定到底使用多大的 k 来作为最终参数。</p>
<p>对于 knn 算法的实现， R 语言中已经有了很多种方式。以下我们介绍其中三种。第一种是通过自己编写代码来实现 knn 算法，其代码来自于 <em>&lt;机器学习:实用案例解析&gt;</em> 一书。另外两种分别借助 R 语言中的两个包，分别是 <strong>class</strong> 包和 <strong>caret</strong> 包。</p>
<hr>
<h2 id="使用自己编写的代码实现_knn_算法">使用自己编写的代码实现 knn 算法</h2>
<h3 id="数据集介绍">数据集介绍</h3>
<p>首先我们使用的是来自于 &lt;机器学习:实用案例解析&gt; 自带的数据，可以点击<a href="https://github.com/johnmyleswhite/ML_for_Hackers/blob/master/10-Recommendations/data/example_data.csv" target="_blank" rel="external">这里</a>下载</p>
<p>首先我们读取数据集，并进行查看</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">data &lt;- read.csv(<span class="string">"example_data.csv"</span>)</div><div class="line">str(data)</div></pre></td></tr></table></figure>



<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="preprocessor">## 'data.frame':	100 obs. of  3 variables:</span></div><div class="line"><span class="preprocessor">##  $ X    : num  2.37 3.18 2.16 4.6 3.33 ...</span></div><div class="line"><span class="preprocessor">##  $ Y    : num  5.4 4.39 5.34 3.87 6.43 ...</span></div><div class="line"><span class="preprocessor">##  $ Label: int  0 0 0 0 0 0 0 0 0 0 ...</span></div></pre></td></tr></table></figure>



<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">summary(data)</div></pre></td></tr></table></figure>



<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">##        <span class="tag">X</span>                <span class="tag">Y</span>             <span class="tag">Label</span>    </div><div class="line">##  <span class="tag">Min</span>.   <span class="pseudo">:0</span><span class="class">.7853</span>   <span class="tag">Min</span>.   <span class="pseudo">:3</span><span class="class">.195</span>   <span class="tag">Min</span>.   <span class="pseudo">:0</span><span class="class">.0</span>  </div><div class="line">##  1<span class="tag">st</span> <span class="tag">Qu</span>.<span class="pseudo">:3</span><span class="class">.0829</span>   1<span class="tag">st</span> <span class="tag">Qu</span>.<span class="pseudo">:5</span><span class="class">.134</span>   1<span class="tag">st</span> <span class="tag">Qu</span>.<span class="pseudo">:0</span><span class="class">.0</span>  </div><div class="line">##  <span class="tag">Median</span> <span class="pseudo">:3</span><span class="class">.9015</span>   <span class="tag">Median</span> <span class="pseudo">:6</span><span class="class">.067</span>   <span class="tag">Median</span> <span class="pseudo">:0</span><span class="class">.5</span>  </div><div class="line">##  <span class="tag">Mean</span>   <span class="pseudo">:3</span><span class="class">.9740</span>   <span class="tag">Mean</span>   <span class="pseudo">:6</span><span class="class">.097</span>   <span class="tag">Mean</span>   <span class="pseudo">:0</span><span class="class">.5</span>  </div><div class="line">##  3<span class="tag">rd</span> <span class="tag">Qu</span>.<span class="pseudo">:4</span><span class="class">.7370</span>   3<span class="tag">rd</span> <span class="tag">Qu</span>.<span class="pseudo">:7</span><span class="class">.013</span>   3<span class="tag">rd</span> <span class="tag">Qu</span>.<span class="pseudo">:1</span><span class="class">.0</span>  </div><div class="line">##  <span class="tag">Max</span>.   <span class="pseudo">:7</span><span class="class">.0872</span>   <span class="tag">Max</span>.   <span class="pseudo">:9</span><span class="class">.308</span>   <span class="tag">Max</span>.   <span class="pseudo">:1</span><span class="class">.0</span></div></pre></td></tr></table></figure>

<p>可以看到该数据集有100个样本点，每个样本点有两个数值型特征，并且其数据分布范围相差不大。第三列是这些样本所归属的类，但是该列现在仍然是 int 类型</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">table(data$Label)</div></pre></td></tr></table></figure>



<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="preprocessor">## </span></div><div class="line"><span class="preprocessor">##  0  1 </span></div><div class="line"><span class="preprocessor">## 50 50</span></div></pre></td></tr></table></figure>

<p>归属于 0 和 1 这两类的样本各为 50 个。由于只有两个特征，我们可以使用 ggplot2 包来对这些样本点进行可视化。很显然，X 和 Y 用来确定样本点的位置，而 Label 用来给点上色。</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">library</span>(ggplot2)</div><div class="line">visual &lt;- ggplot(data, aes(X,Y,colour = factor(Label)))</div><div class="line">visual + geom_point() + scale_colour_brewer(palette = <span class="string">"Set1"</span>)</div></pre></td></tr></table></figure>

<p><img src="/img/knn/colour.png" alt="colour">  </p>
<p>可以看到，不同 Label 的点相对较为分离，不过仍有一些交叉。我们使用 knn 算法，可能会得到相对不错的效果。</p>
<hr>
<h3 id="距离矩阵定义">距离矩阵定义</h3>
<p>由于数据在二维平面上作图得到良好的感官效果，我们可以顺理成章的使用二维的欧氏距离来定义两个样本点之间的距离。在下面的代码中，我们给 distance.matrix 函数传入一个数据框 <code>df</code>。该函数会先创建一个 <code>nrow(df)</code> 行 <code>nrow(df)</code> 列的方阵，并使用循环遍历方阵中的每一个元素，将其赋值为对应位置两个样本点的欧氏距离。该函数最后返回距离矩阵。矩阵的第 <code>(i,j)</code> 位置的值，就是第 i 个样本点和第 j  个样本点的欧式距离。</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">distance.matrix &lt;- <span class="keyword">function</span>(df)</div><div class="line">{</div><div class="line">  distance &lt;- matrix(rep(<span class="literal">NA</span>, nrow(df) ^ <span class="number">2</span>), nrow = nrow(df))</div><div class="line">  </div><div class="line">  <span class="keyword">for</span> (i <span class="keyword">in</span> <span class="number">1</span>:nrow(df))</div><div class="line">  {</div><div class="line">    <span class="keyword">for</span> (j <span class="keyword">in</span> <span class="number">1</span>:nrow(df))</div><div class="line">    {</div><div class="line">      distance[i, j] &lt;- sqrt((df[i, <span class="string">'X'</span>] - df[j, <span class="string">'X'</span>]) ^ <span class="number">2</span> + (df[i, <span class="string">'Y'</span>] - df[j, <span class="string">'Y'</span>]) ^ <span class="number">2</span>)</div><div class="line">    }</div><div class="line">  }</div><div class="line"></div><div class="line">  <span class="keyword">return</span>(distance)</div><div class="line">}</div></pre></td></tr></table></figure>

<hr>
<h3 id="knn_函数的定义">knn 函数的定义</h3>
<p>有了距离矩阵，我们对任意的一个样本点，比如说第 i 个样本点，就能知道其余所有点和它的距离了。那么我们自然也可以对这些距离排个序，并找出离 i 最近的 k 个样本点来。这里需要注意的是，由于样本点 i 和自身的距离永远为零，那么我们在使用最近的 k 个邻居来对其进行预测时，当然不能把它自己给算进去了。所以距离最小的这个自身的点，我们用不上。那么有了以上的思路以后，我们就有了下面的函数，该函数用来对样本点 i 找出和其最近的 k 个样本来(不包括它自己)，并返回这些样本点的位置。</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">k.nearest.neighbors &lt;- <span class="keyword">function</span>(i, distance, k = <span class="number">5</span>)</div><div class="line">{</div><div class="line">  <span class="keyword">return</span>(order(distance[i, ])[<span class="number">2</span>:(k + <span class="number">1</span>)])</div><div class="line">}</div></pre></td></tr></table></figure>

<p>有了以上两个函数做铺垫，我们可以定义最后的 knn 函数了。该函数有两个参数，第一个是数据框，第二个是选择的最近邻居个数 k。当数据框进入 knn 函数后，我们先使用 <code>distance.matrix</code> 函数计算出其距离矩阵。然后我们使用 <code>k.nearest.neighbors</code> 函数，对数据框中的每一个样本，找出和其最近的 k 个样本点，然后计算这 k 个样本点的 Label 均值。由于 Label 只有 1 和 0 两个值，那么近邻中取哪个值的样本点占得比例高，Label 的均值就应该在 0.5 的基础上向着相应值靠拢，这就是下面利用 <code>ifelse</code> 函数对样本点的 Label 进行预测的依据这种决策方法也叫做 <strong>majority vote</strong>，或者叫<strong>草根民主</strong>。最后，knn 函数返回所有样本点的一个 Label 预测。我们可以用该预测和所有点的 Label 真实值作比较，看看我们的算法的正确率如何。</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">knn &lt;- <span class="keyword">function</span>(df, k = <span class="number">5</span>)</div><div class="line">{</div><div class="line">  distance &lt;- distance.matrix(df)</div><div class="line">  </div><div class="line">  predictions &lt;- rep(<span class="literal">NA</span>, nrow(df))</div><div class="line">  </div><div class="line">  <span class="keyword">for</span> (i <span class="keyword">in</span> <span class="number">1</span>:nrow(df))</div><div class="line">  {</div><div class="line">    indices &lt;- k.nearest.neighbors(i, distance, k = k)</div><div class="line">    predictions[i] &lt;- ifelse(mean(df[indices, <span class="string">"Label"</span>]) &gt; <span class="number">0.5</span>, <span class="number">1</span>, <span class="number">0</span>)</div><div class="line">  }</div><div class="line">  </div><div class="line">  <span class="keyword">return</span>(predictions)</div><div class="line">}</div></pre></td></tr></table></figure>

<hr>
<h3 id="正确率的计算">正确率的计算</h3>
<p>将 knn 函数返回的 Label 预测向量利用 <code>transform</code> 函数添加到原数据框中</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">data &lt;- transform(data, kNNPredictions = knn(data))</div></pre></td></tr></table></figure>

<p>预测错误了多少条？</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sum(with(data, Label != kNNPredictions))</div></pre></td></tr></table></figure>



<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="preprocessor">## [1] 7</span></div></pre></td></tr></table></figure>

<p>knn 算法的正确率多少？</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sum(with(data, Label == kNNPredictions))/nrow(data)</div></pre></td></tr></table></figure>



<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="preprocessor">## [1] 0.93</span></div></pre></td></tr></table></figure>

<p>可以看到，我们的正确率还是不错的，不过仍然有一些点是预测错误的。具体是哪些点呢？</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">with(data,which(Label != kNNPredictions))</div></pre></td></tr></table></figure>



<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="preprocessor">## [1]  5 11 18 20 84 93 95</span></div></pre></td></tr></table></figure>

<p>我们可以将这些点在图中标注出来，用三角形来表示预测错误的点</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">index &lt;- with(data,which(Label != kNNPredictions))</div><div class="line">data$differ &lt;- <span class="string">"right"</span></div><div class="line">data$differ[index] &lt;- <span class="string">"wrong"</span></div><div class="line">visual + geom_point(aes(shape = factor(data$differ))) + scale_shape_manual(values = c(<span class="number">21</span>,<span class="number">24</span>)) + scale_colour_brewer(palette = <span class="string">"Set1"</span>)</div></pre></td></tr></table></figure>

<p><img src="/img/knn/wrong.png" alt="wrong"> </p>
<p>可以看到，预测错误的点都是“深入敌后”的样本点。</p>
<hr>
<h2 id="使用_class_包实现_knn_算法">使用 class 包实现 knn 算法</h2>
<p>class 包中有三个和 knn 算法相关的函数，分别是</p>
<ul>
<li>knn k-Nearest Neighbour Classification</li>
<li>knn1 1-nearest neighbour classification</li>
<li>knn.cv k-Nearest Neighbour Cross-Validatory Classification</li>
</ul>
<p>关于三个函数的参数说明和使用方法，请见<a href="http://ripeconan.com/2014/11/10/class_knn/" target="_blank" rel="external">class 包中的 knn 系列函数</a>一文</p>
<hr>
<h3 id="数据准备">数据准备</h3>
<p>我们仍然采用前面的数据集 <code>data</code></p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">head(data)</div></pre></td></tr></table></figure>



<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="preprocessor">##          X        Y Label kNNPredictions differ</span></div><div class="line"><span class="preprocessor">## 1 2.373546 5.398106     0              0  right</span></div><div class="line"><span class="preprocessor">## 2 3.183643 4.387974     0              0  right</span></div><div class="line"><span class="preprocessor">## 3 2.164371 5.341120     0              0  right</span></div><div class="line"><span class="preprocessor">## 4 4.595281 3.870637     0              0  right</span></div><div class="line"><span class="preprocessor">## 5 3.329508 6.433024     0              1  wrong</span></div><div class="line"><span class="preprocessor">## 6 2.179532 6.980400     0              0  right</span></div></pre></td></tr></table></figure>

<p>我们只保留原始数据，因此去掉 <code>data</code> 数据集的后两列</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">data &lt;- data[c(<span class="string">"X"</span>,<span class="string">"Y"</span>,<span class="string">"Label"</span>)]</div><div class="line">head(data)</div></pre></td></tr></table></figure>



<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="preprocessor">##          X        Y Label</span></div><div class="line"><span class="preprocessor">## 1 2.373546 5.398106     0</span></div><div class="line"><span class="preprocessor">## 2 3.183643 4.387974     0</span></div><div class="line"><span class="preprocessor">## 3 2.164371 5.341120     0</span></div><div class="line"><span class="preprocessor">## 4 4.595281 3.870637     0</span></div><div class="line"><span class="preprocessor">## 5 3.329508 6.433024     0</span></div><div class="line"><span class="preprocessor">## 6 2.179532 6.980400     0</span></div></pre></td></tr></table></figure>

<p>由于我们在上文中已经定义了 knn 函数，这与即将要加载的 <strong>class</strong> 包中 knn 函数同名，因此我们需要先删除之前定义的 knn 函数，以免引起混淆</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">rm(knn)</div><div class="line"><span class="keyword">library</span>(class)</div></pre></td></tr></table></figure>

<p>接下来，我们为 class 包中的 knn 函数和 knn1 函数准备训练集和测试集</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">trainset &lt;- data[c(<span class="number">1</span>:<span class="number">25</span>,<span class="number">51</span>:<span class="number">75</span>),<span class="number">1</span>:<span class="number">2</span>]</div><div class="line">testset &lt;- data[c(<span class="number">26</span>:<span class="number">50</span>,<span class="number">76</span>:<span class="number">100</span>),<span class="number">1</span>:<span class="number">2</span>]</div><div class="line">trainlabel &lt;- data[c(<span class="number">1</span>:<span class="number">25</span>,<span class="number">51</span>:<span class="number">75</span>),<span class="number">3</span>]</div></pre></td></tr></table></figure>

<hr>
<h3 id="使用_class::knn_和_class::knn1_进行分类">使用 class::knn 和 class::knn1 进行分类</h3>
<p>针对上述测试集，我们对其分类进行预测</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">pred.knn &lt;- knn(train = trainset,test = testset,cl = trainlabel, k = <span class="number">3</span>)</div><div class="line">pred.knn1 &lt;- knn1(train = trainset,test = testset,cl = trainlabel)</div></pre></td></tr></table></figure>

<p>我们来看一下使用 knn 和 knn1 的各自分类效果如何</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">testlabel &lt;- data[c(<span class="number">26</span>:<span class="number">50</span>,<span class="number">76</span>:<span class="number">100</span>),<span class="number">3</span>]</div><div class="line">accuracy.knn &lt;- sum(pred.knn == testlabel)/length(testlabel)</div><div class="line">accuracy.knn1 &lt;- sum(pred.knn1 == testlabel)/length(testlabel)</div><div class="line">accuracy.knn</div></pre></td></tr></table></figure>



<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="preprocessor">## [1] 0.86</span></div></pre></td></tr></table></figure>



<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">accuracy.knn1</div></pre></td></tr></table></figure>



<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="preprocessor">## [1] 0.86</span></div></pre></td></tr></table></figure>

<hr>
<h3 id="使用_class::knn-cv_进行分类">使用 class::knn.cv 进行分类</h3>
<p>对于 knn.cv 函数，我们把所有的数据都放到训练集中去</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">trainset &lt;- data[<span class="number">1</span>:<span class="number">100</span>,<span class="number">1</span>:<span class="number">2</span>]</div><div class="line">trainlabel &lt;- data[<span class="number">1</span>:<span class="number">100</span>,<span class="number">3</span>]</div></pre></td></tr></table></figure>

<p>使用 knn.cv 进行预测，并计算其准确率</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sum(knn.cv(trainset,trainlabel,k = <span class="number">3</span>) == trainlabel)/length(trainlabel)</div></pre></td></tr></table></figure>



<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="preprocessor">## [1] 0.91</span></div></pre></td></tr></table></figure>

<p>可以看到，同样使用 k = 3 的参数，knn.cv 比 knn 的表现要好一些</p>
<hr>
<h2 id="使用_caret_包实现_knn_算法">使用 caret 包实现 knn 算法</h2>
<p>仍然使用上述数据集 <code>data</code>, 我们使用 caret 包中的 createDataPartition 函数将数据集划分为训练集和测试集，然后使用 caret 包中的 knn3 函数和 predict.knn3 函数进行分类预测，并最终计算分类准确率</p>
<hr>
<h3 id="使用_createDataPartition_函数划分训练集和测试集">使用 createDataPartition 函数划分训练集和测试集</h3>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">library</span>(caret)</div></pre></td></tr></table></figure>



<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">## Loading <span class="keyword">required</span> <span class="keyword">package</span>: lattice</div></pre></td></tr></table></figure>



<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">set.seed(<span class="number">2014</span>)</div><div class="line">createDataPartition(y = data$Label, times = <span class="number">1</span>, p = <span class="number">0.5</span>)</div></pre></td></tr></table></figure>



<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="preprocessor">## $Resample1</span></div><div class="line"><span class="preprocessor">##  [1]   2   3   4   5   7   9  15  17  20  22  23  24  26  31  32  35  36</span></div><div class="line"><span class="preprocessor">## [18]  38  40  41  43  44  46  47  50  52  57  58  61  62  65  67  69  70</span></div><div class="line"><span class="preprocessor">## [35]  72  74  76  77  78  79  83  87  88  89  90  93  96  98  99 100</span></div></pre></td></tr></table></figure>

<p>在 createDataPartition 函数中，主要有三个参数(其他参数请查阅帮助文档)</p>
<ul>
<li>y 真实分类数据</li>
<li>times 从 y 中创建的 partition 的个数，除非重复实验，否则需要一个就行</li>
<li>p 训练集占数据集的比重</li>
</ul>
<p>使用 createDataPartition 的好处在于，它能将低熵数据集随机抽取出我们需要的训练集来。比如我们的数据集共有 100 个样本点，前 50 个是一类，后 50 个是一类。我们为了让训练集里两类样本都各有一些，必然希望从前 50 个样本点随机抽取一定比例，后 50 个里也随机抽取相应比例的样本点来组成训练集。这个手动过程因为涉及到人的主观意识，从而不能保证完全随机化。而 createDataPartition 会自动从 y 的各个 level 随机取出等比例的数据来，组成训练集，给我们省了很多事。</p>
<p>createDataPartition 函数默认返回的是列表，我们将其中的训练集对应坐标提取出来，并用其去划分训练集</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">set.seed(<span class="number">2014</span>)</div><div class="line">index.caret &lt;- createDataPartition(y = data$Label, times = <span class="number">1</span>, p = <span class="number">0.5</span>)[[<span class="number">1</span>]]</div></pre></td></tr></table></figure>

<hr>
<h3 id="使用_knn3_函数进行分类预测">使用 knn3 函数进行分类预测</h3>
<p>这一步要使用 caret 包的两个函数</p>
<ul>
<li>knn3 用来建模，有好几种实现形式，我们选用公式形式，返回的是一个 knn3 类型</li>
<li>predict.knn3，也可以写成 predict，针对 knn3 类型的模型，提供测试数据集，可以预测</li>
</ul>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">data.fit &lt;- knn3(factor(Label) ~ ., data, index.caret, k = <span class="number">3</span>)</div><div class="line">predict.caret &lt;- predict(data.fit,newdata = data[-index.caret,<span class="number">1</span>:<span class="number">2</span>],type = <span class="string">"class"</span>)</div><div class="line">predict.caret</div></pre></td></tr></table></figure>



<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="preprocessor">##  [1] 0 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1</span></div><div class="line"><span class="preprocessor">## [36] 1 1 1 1 1 1 1 0 1 0 1 1 1 0 1</span></div><div class="line"><span class="preprocessor">## Levels: 0 1</span></div></pre></td></tr></table></figure>

<p>我们再把预测的结果和测试集的真实分类作比较，计算预测准确率</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sum(predict.caret == data[-index.caret,<span class="number">3</span>])/length(index.caret)</div></pre></td></tr></table></figure>



<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="preprocessor">## [1] 0.88</span></div></pre></td></tr></table></figure>

<p>准确率也不错。</p>
<hr>
<p>综上所述，在本篇文章中的数据集下，用自己编写代码的方式、class包、caret包都能取得较好的预测准确率。</p>
]]></content>
         
         
           
             
              <breadCrumb title="Machine Learning" url="http://ripeconan.com/categories/Machine-Learning/"/>
          
        </display>
    </data>
    </url>

    
  <url>
    <loc>http://ripeconan.com/2014/11/10/class_knn/</loc>
    <lastmod>2014-11-10T16:21:27.000Z</lastmod>
    <data>
        <display>
        <title>class 包中的 knn 系列函数</title>
        <pubTime>2014-11-09T16:00:00.000Z</pubTime>
        
        <tag>R package </tag>
         
        <tag>R函数 </tag>
         
         <content><![CDATA[<h2 id="函数功能">函数功能</h2>
<p>对测试集的每一行，去找训练集里欧式距离最近的 k 行，利用草根民主法，决定测试集该行的类别。</p>
<p><strong>class</strong> 包中还有两个和 <strong>knn</strong> 相关的函数：<strong>knn1</strong>, <strong>knn.cv</strong><br>这三个包处理的问题分别是：</p>
<ul>
<li>knn k-Nearest Neighbour Classification</li>
<li>knn1 1-nearest neighbour classification</li>
<li>knn.cv k-Nearest Neighbour Cross-Validatory Classification</li>
</ul>
<hr>
<h2 id="函数参数说明">函数参数说明</h2>
<h3 id="knn_的函数说明"><strong>knn</strong> 的函数说明</h3>
<p><strong>knn</strong> 函数形式是  <code>knn(train, test, cl, k = 1, l = 0, prob = FALSE, use.all = TRUE)</code> , 重要的参数如下：</p>
<ul>
<li>train 训练集数据，可以是矩阵或者数据框类型</li>
<li>test 测试集数据，可以是矩阵或者数据框类型</li>
<li>cl 训练集对应的真实分类数据，应该是因子类型的向量</li>
<li>k 选择的近邻个数</li>
</ul>
<hr>
<h3 id="knn1_的函数说明"><strong>knn1</strong> 的函数说明</h3>
<p><strong>knn1</strong> 函数形式是 <code>knn1(train, test, cl)</code>，即 <strong>knn</strong> 中选择 <code>k = 1</code> 的情况，因此 <strong>knn1</strong> 是 <strong>knn</strong> 的一个退化情况 </p>
<hr>
<h3 id="knn-cv_的函数说明"><strong>knn.cv</strong> 的函数说明</h3>
<p><strong>knn.cv</strong> 函数形式是  <code>knn.cv(train, cl, k = 1, l = 0, prob = FALSE, use.all = TRUE)</code>，其使用的方法是 <strong>leave-one-out cross validation</strong>。即所有的样本点都算到训练集中去，不设置测试集。对每个训练集中的样本点，都使用剩余的样本点中的 k 个最近邻来进行投票，从而决定该样本点的分类。</p>
<hr>
<h2 id="函数返回值说明">函数返回值说明</h2>
<p>该系列函数返回一个<strong>因子</strong>向量，是测试集对应的分类(在 <strong>knn.cv</strong> 中是训练集对应的分类)</p>
<hr>
<h2 id="函数使用示例">函数使用示例</h2>
<h3 id="数据集情况">数据集情况</h3>
<p>我们使用 R 自带的 <code>iris3</code> 数据集，先看看数据大致情况</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">class(iris3)</div></pre></td></tr></table></figure>



<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">#<span class="array"># </span>[<span class="number">1</span>] <span class="string">"array"</span></div></pre></td></tr></table></figure>



<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">dim(iris3)</div></pre></td></tr></table></figure>



<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="preprocessor">## [1] 50  4  3</span></div></pre></td></tr></table></figure>

<p>数据是以三维数组的形式存放，第三维度即3种鸢尾属植物，对于每一种植物，我们采集了4个特征，分别是</p>
<p>-Sepal.Length 花萼长度<br>-Sepal.Width 花萼宽度<br>-Petal.Length 花瓣长度<br>-Petal.Width 花瓣宽度</p>
<p>每种植物，我们采集了50个样本</p>
<hr>
<h3 id="数据集处理">数据集处理</h3>
<p>我们对每种植物各选择25个样本，组成训练集，因此训练集的大小是75个样本</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">train &lt;- rbind(iris3[<span class="number">1</span>:<span class="number">25</span>,,<span class="number">1</span>], iris3[<span class="number">1</span>:<span class="number">25</span>,,<span class="number">2</span>], iris3[<span class="number">1</span>:<span class="number">25</span>,,<span class="number">3</span>])</div></pre></td></tr></table></figure>

<p>同样，每种植物剩下的25个样本，组合起来，作为测试集</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">test &lt;- rbind(iris3[<span class="number">26</span>:<span class="number">50</span>,,<span class="number">1</span>], iris3[<span class="number">26</span>:<span class="number">50</span>,,<span class="number">2</span>], iris3[<span class="number">26</span>:<span class="number">50</span>,,<span class="number">3</span>])</div></pre></td></tr></table></figure>

<p>再准备好训练集的分类数据向量，要求是因子类型，所以要转化一下</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">cl &lt;- factor(c(rep(<span class="string">"s"</span>,<span class="number">25</span>), rep(<span class="string">"c"</span>,<span class="number">25</span>), rep(<span class="string">"v"</span>,<span class="number">25</span>)))</div></pre></td></tr></table></figure>

<hr>
<h3 id="使用_knn_预测">使用 knn 预测</h3>
<p>最后，使用 <strong>knn</strong> 函数，选择最邻近的 3 个样本点来投票</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">library</span>(class)</div><div class="line">final &lt;- knn(train, test, cl, k = <span class="number">3</span>)</div><div class="line">final</div></pre></td></tr></table></figure>



<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">##  [<span class="number">1</span>] s s s s s s s s s s s s s s s s s s s s s s s s s <span class="built_in">c</span> <span class="built_in">c</span> v <span class="built_in">c</span> <span class="built_in">c</span> <span class="built_in">c</span> <span class="built_in">c</span> <span class="built_in">c</span> v <span class="built_in">c</span></div><div class="line">## [<span class="number">36</span>] <span class="built_in">c</span> <span class="built_in">c</span> <span class="built_in">c</span> <span class="built_in">c</span> <span class="built_in">c</span> <span class="built_in">c</span> <span class="built_in">c</span> <span class="built_in">c</span> <span class="built_in">c</span> <span class="built_in">c</span> <span class="built_in">c</span> <span class="built_in">c</span> <span class="built_in">c</span> <span class="built_in">c</span> <span class="built_in">c</span> v <span class="built_in">c</span> <span class="built_in">c</span> v v v v v v v v v v <span class="built_in">c</span> v v v v v v</div><div class="line">## [<span class="number">71</span>] v v v v v</div><div class="line">## <span class="type">Levels</span>: <span class="built_in">c</span> s v</div></pre></td></tr></table></figure>

<p>我们可以看一下正确率如何</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="number">1</span> - sum(cl != final)/length(cl)</div></pre></td></tr></table></figure>



<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="preprocessor">## [1] 0.9333333</span></div></pre></td></tr></table></figure>

<hr>
<h3 id="使用_knn1_预测">使用 knn1 预测</h3>
<p>如果使用 knn1 函数进行预测，则选择的就是最近的一个样本点来直接做决定</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">final1 &lt;- knn1(train, test, cl)</div><div class="line"><span class="number">1</span> - sum(cl != final1)/length(cl)</div></pre></td></tr></table></figure>



<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="preprocessor">## [1] 0.9466667</span></div></pre></td></tr></table></figure>

<p>可以看到，<strong>knn1</strong> 在本例中的正确率要高于 k = 3 时的 <strong>knn</strong> </p>
<hr>
<h3 id="使用_knn-cv_预测">使用 knn.cv 预测</h3>
<p>由于在 <strong>knn.cv</strong> 中不设置测试集，所以要把所有的样本点都放到训练集中去，因此训练集共 150 个样本点</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">train &lt;- rbind(iris3[,,<span class="number">1</span>], iris3[,,<span class="number">2</span>], iris3[,,<span class="number">3</span>])</div></pre></td></tr></table></figure>

<p>同样，训练集对应的真实分类数据也是长度为 150 的向量</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">cl.cv &lt;- factor(c(rep(<span class="string">"s"</span>,<span class="number">50</span>), rep(<span class="string">"c"</span>,<span class="number">50</span>), rep(<span class="string">"v"</span>,<span class="number">50</span>)))</div></pre></td></tr></table></figure>

<p>最后，对训练集的每一行做预测，利用其余行的 3 个最近邻来投票</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">final.cv &lt;- knn.cv(train, cl.cv, k = <span class="number">3</span>)</div><div class="line"><span class="number">1</span> - sum(cl.cv != final.cv)/length(cl.cv)</div></pre></td></tr></table></figure>



<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="preprocessor">## [1] 0.96</span></div></pre></td></tr></table></figure>

<p>可以看到，iris3 中对这三种方法的预测效果排序，<strong>knn.cv(k = 3)</strong> 优于 <strong>knn1</strong> 优于 <strong>knn(k = 3)</strong></p>
]]></content>
         
         
           
             
              <breadCrumb title="Machine Learning" url="http://ripeconan.com/categories/Machine-Learning/"/>
          
        </display>
    </data>
    </url>

    
  <url>
    <loc>http://ripeconan.com/2014/10/23/recent%20to%20do/</loc>
    <lastmod>2014-10-23T01:29:39.000Z</lastmod>
    <data>
        <display>
        <title>近期学习内容安排:2014年第四季度</title>
        <pubTime>2014-10-23T01:19:55.000Z</pubTime>
        
        <tag>To Do </tag>
         
         <content><![CDATA[<h2 id="书目阅读">书目阅读</h2>
<ul>
<li><p>统计科普类</p>
<ol>
<li>无处不在的统计</li>
<li>漫画统计学入门</li>
<li>趣话概率——兼话《红楼梦》中的玄机</li>
<li>漫话信息时代的统计学</li>
<li>爱上统计学</li>
<li>统计数字会撒谎</li>
</ol>
</li>
<li><p>R语言编程类</p>
<ol>
<li>统计模拟及其R实现</li>
<li>ggplot2:数据分析与图形艺术</li>
<li>数据挖掘与R语言</li>
<li>R语言实战</li>
<li>R语言经典实例</li>
<li>R数据可视化手册</li>
<li>统计建模与R软件</li>
<li>R语言与统计分析</li>
<li>复杂数据统计方法——基于R的应用</li>
</ol>
</li>
<li><p>Python编程类</p>
<ol>
<li>Python语言程序设计:英文版.影印版</li>
<li>Natural language processing with python.英文影印版</li>
<li>NumPy攻略:Python科学计算与数据分析</li>
<li>统计思维:程序员数学之概率统计</li>
</ol>
</li>
<li><p>其他数据分析类</p>
<ol>
<li>数据之魅:基于开源工具的数据分析</li>
<li>驾驭大数据</li>
<li>大数据:互联网大规模数据挖掘与分布式处理</li>
</ol>
</li>
<li><p>概率统计理论类</p>
<ol>
<li>概率论基础教程</li>
<li>概率论及其应用</li>
<li>深入浅出数据分析</li>
<li>Survival Analysis</li>
<li>Logistic Regression</li>
</ol>
</li>
</ul>
<hr>
<h2 id="MOOC学习">MOOC学习</h2>
<ol>
<li>数据分析与统计推断(Duke)</li>
<li>Python交互编程入门(Rice)</li>
<li>机器学习基石(NTU)</li>
<li>海量数据挖掘(Stanford)</li>
</ol>
<hr>
<h2 id="研究方向">研究方向</h2>
<ol>
<li>半参数模型的统计模拟</li>
<li>非独立随机变量的统计模拟</li>
<li>机器学习在实时交通预测上的应用</li>
<li>通过API来获取实时数据并加工整理</li>
</ol>
]]></content>
         
         
           
             
              <breadCrumb title="Learning" url="http://ripeconan.com/categories/Learning/"/>
          
        </display>
    </data>
    </url>

    
  <url>
    <loc>http://ripeconan.com/2014/10/22/Power%20Map/</loc>
    <lastmod>2014-10-22T14:19:15.000Z</lastmod>
    <data>
        <display>
        <title>Power Map 学习笔记</title>
        <pubTime>2014-10-21T16:00:00.000Z</pubTime>
        
        <tag>Excel </tag>
         
        <tag>可视化 </tag>
         
         <content><![CDATA[<h2 id="Power_Map_介绍">Power Map 介绍</h2>
<p><strong>Power Map</strong> 是微软2013年推出的 Excel2013 的插件， <strong>Power BI</strong> 的组件之一，其前身是 <strong>Geoflow</strong>。以下是 <strong>Power Map</strong> 的官方简介：</p>
<blockquote>
<p>Power Map is a new 3D visualization add-in for Excel for mapping, exploring,<br>and interacting with geographical and temporal data, enabling people to<br>discover and share new insights.</p>
</blockquote>
<p>点击<a href="http://www.microsoft.com/en-us/download/details.aspx?id=38395" target="_blank" rel="external">下载链接</a>进入下载页面，安装较为容易，也可以参考<a href="http://jingyan.baidu.com/album/03b2f78c4f3fac5ea237ae06.html" target="_blank" rel="external">安装教程</a></p>
<hr>
<h2 id="横向数据的展示（与时间无关）">横向数据的展示（与时间无关）</h2>
<p><strong>Power Map</strong> 主要应用于具有地理位置信息的数据，其可以接受的地理位置有：  </p>
<ul>
<li>国家/地区</li>
<li>省/市/自治区</li>
<li>城市</li>
<li>经纬度</li>
<li>邮政编码 </li>
<li>…… </li>
</ul>
<p>为了方便起见，我们选用以下形式的 <em>Excel</em> 数据集来进行测试：<br><img src="/img/powermap/1.png" alt="横向数据"></p>
<p>选中数据所在单元格之后，点击 <code>插入——启动 Power Map</code>，进入地图界面：<br><img src="/img/powermap/2.png" alt="地图界面"></p>
<p>按照右下方提示，将<strong>地区</strong>变量选择为<strong>省/市/自治区</strong>，进入下一步，<br>则地图会旋转到地区变量匹配的区域：<br><img src="/img/powermap/3.png" alt="地区匹配"></p>
<p>此时进入到选择<strong>绘制变量</strong>的阶段，我们需要从已有变量中勾选出我们想要展示的变量（右上角）。对于右下角的界面，也有变化：<br><img src="/img/powermap/4.png" alt="可视化选项"></p>
<p>上面五个图形分别是： </p>
<ul>
<li>堆积柱形图(Stacked)</li>
<li>簇状柱形图(Clustered)</li>
<li>气泡图(Bubble)</li>
<li>热度地图(Heat)</li>
<li>将可视化转为区域</li>
</ul>
<p>前四个图形对应的是不同的数据展现形式，而展现的位置，是地区变量对应的一个点(例如，地区变量是省的名称，则数据就会到该省的省会城市所在地的点进行绘制)；而最后一种<strong>将可视化转为区域</strong>，则是将绘制范围从点转为面(即在该省所在面积上进行绘制)。下面将举例说明。</p>
<hr>
<h3 id="柱形图">柱形图</h3>
<p>对于柱形图的 <strong>高度、类别、时间</strong> 这三个需要填补的字段(或者称为变量)，我们应该分别填入<strong>数值型变量、分类变量、日期时间变量</strong>。在本例中，<em>变量1</em>和<em>变量2</em>是数值型变量，所以我们可以将其添加到高度中，以下是<strong>堆积柱形图</strong>和<strong>簇状柱形图</strong>的效果：<br><img src="/img/powermap/stack.png" alt="堆积柱形图"><br><img src="/img/powermap/cluster.png" alt="簇状柱形图"></p>
<p>这里面有三点值得注意：  </p>
<ol>
<li>两张地图的视角不一，这是因为我们可以使用地图的旋转和拉伸等方式来操作 <img src="/img/powermap/rotate.png" alt="旋转按钮"></li>
<li>右上角默认显示了图例，我们可以对其进行编辑，如图层名称，柱形颜色</li>
<li>我们将变量的数值展示方式由默认的<strong>求和</strong>改为了<strong>无聚合</strong>，其他还有几种，如：<ul>
<li>平均</li>
<li>计数</li>
<li>最大值</li>
<li>最小值</li>
</ul>
</li>
</ol>
<hr>
<h3 id="气泡图">气泡图</h3>
<p>对于气泡图，我们可以让气泡的大小表示单个变量的值的大小，比如：</p>
<p><img src="/img/powermap/bubble1.png" alt="单变量气泡图"></p>
<p>也可以使用气泡图展示多个变量值。此时，让不同地区的气泡维持同样大小，但气泡中不同颜色比例代表了该地区不同的值的大小关系，具体大家可以尝试一下。我们还可以在右边的设置窗口中，调节气泡的<strong>不透明度、大小、厚度</strong></p>
<p><img src="/img/powermap/bubble2.png" alt="多变量气泡图"></p>
<hr>
<h3 id="热度图">热度图</h3>
<p>热度图只能展示不同地区单变量的值，根据图例我们可以看到颜色和值的关联性，进而通过颜色的变化来了解地图上不同地区的变量值的大小。同样，使用设置窗口，我们可以更改热度图的<strong>不透明度、色阶、影响半径</strong></p>
<p><img src="/img/powermap/heat.png" alt="热度图"></p>
<hr>
<h3 id="将可视化更改为区域">将可视化更改为区域</h3>
<p>前面提到过，这个选项是将绘制范围从点转为面，按照各个地区的轮廓，在其内进行数值展示。如果对单变量的值进行绘制的话，则颜色和值的大小相关。关于此选项还有一些可以深入调试的地方，有兴趣的可以自行尝试。</p>
<p><img src="/img/powermap/region.png" alt="区域图"></p>
<hr>
<h2 id="纵向数据的展示（按时间变化）">纵向数据的展示（按时间变化）</h2>
<h3 id="数据集选取">数据集选取</h3>
<p><strong>Power Map</strong> 在横截面时间上的数据展示只是静态的，其更强大的功能在于<strong>动态可视化</strong>，即引入日期时间变量，让数据动起来。我们使用如下的数据集来说明 <strong>Power Map</strong> 对纵向数据的展示：</p>
<p><img src="/img/powermap/longitudinal.png" alt="纵向数据"></p>
<p>对该数据的说明：</p>
<ol>
<li>地区共有六个：河北、山东、山西、内蒙古、贵州、云南，后面四个地区的变量数据与前面两个相同</li>
<li>日期变量的数据格式为 <strong>日期</strong>，可以点击 <strong>右键——设置单元格格式</strong> 来修改</li>
</ol>
<hr>
<h3 id="引入时间变量来生成动画">引入时间变量来生成动画</h3>
<p>仍然将<strong>地区</strong>变量选择为<strong>省/市/自治区</strong>，进入下一步。勾选 <strong>日期</strong> 变量，由于其格式为 Excel 可识别的日期格式，所以会自动进入到右下方的 <strong>时间</strong> 选项中，注意到此时地图左上角有了时间戳：</p>
<p><img src="/img/powermap/time1.png" alt="时间设置"></p>
<p>然后勾选想要展示的变量，对于数值型变量，会直接进入到 <strong>高度</strong> 选项中。在本例中，我们勾选 <strong>变量1</strong> 和 <strong>变量2</strong>，并将展示方式设置为 <strong>簇状柱形图</strong>，对变量的操作选择 <strong>无聚合</strong></p>
<p><img src="/img/powermap/timeplay.png" alt="时间播放"></p>
<p>注意到地图下方有一个类似于播放器的播放按钮，点击即可进入播放</p>
<iframe height="498" width="510" src="http://player.youku.com/embed/XODA5MTY2MTcy" frameborder="0" allowfullscreen></iframe>

<p>可以看到，随着左上角时间的推移，我们的柱形图也在变化，这就实现了动态的数据可视化。<br>如果对播放速度不满意，我们还可以通过右上方的设置按钮进行多种播放条件进行调整：</p>
<p><img src="/img/powermap/timeconfig.png" alt="播放设置"></p>
<p>值得一提的是，<strong>效果</strong> 选项的调整，只有在点击 Excel 左上角的 <strong>播放演示</strong> 按钮后才能看到最终的效果。</p>
<hr>
<h3 id="时间变量调整">时间变量调整</h3>
<p>如果我们的数据并非以日为单位来采集的话， <strong>Power Map</strong> 该如何对其进行展示？假设我们将刚才的数据集的日期变量进行一下修改：</p>
<p><img src="/img/powermap/timechange.png" alt="时间调整"></p>
<p>此数据集仍然具有六个地区，与刚才数据集的唯一区别，就是日期变量变为<strong>以月为单位</strong>了，但日期变量的格式还是 Excel 可识别的日期格式。  </p>
<p>对此数据集进行<strong>地区选择——时间选择——高度选择</strong>这一系列流程后，我们可以对其进行播放。可以看到，时间的流逝速度加快，我们仍然在默认设置的时间内(我的默认设置是播放6秒)，观看完从2014-1-1至2014-10-1的整个动画。可以预想到，随着我们数据量的增大，当日期变量取值更加紧密的时候(比如每天都有数据点，整个数据的日期跨度为1年)，我们可能需要延长动画播放时间，才能较好的理解数据变化的模式。</p>
<hr>
<h3 id="其他的时间格式">其他的时间格式</h3>
<p>如果我们的数据是类似于传感器采集的更高频率的数据，该怎么操作？对于精确到秒的时间变量，我们只需要注意将时间变量数据格式转为 Excel 可识别的即可，如下图：</p>
<p><img src="/img/powermap/hour.png" alt="小时"></p>
<p>注意到，虽然日期变量的显示格式只是<strong>5:00:00</strong>，但是 Excel 中实际存储的是<strong>2014/1/1  5:00:00</strong>。对此数据进行动态可视化操作也是可行的，流程如上。</p>
<hr>
<h2 id="其他功能">其他功能</h2>
<p>事实上， <strong>Power Map</strong> 还有一些其他功能值得说一下：</p>
<ol>
<li><p>主题的选取。 <strong>Power Map</strong> 提供了多种地图背景供选择，如：<img src="/img/powermap/theme.png" alt="主题"></p>
</li>
<li><p>平面地图。通过点击菜单栏上的<strong>平面地图</strong>，我们可以讲地图设置为平面的。不过旋转和拉伸动作仍然可以操作，并且各种图形的绘制均无影响。<img src="/img/powermap/2d.png" alt="平面地图"></p>
</li>
<li><p>添加图层。类似于 <strong>Photoshop</strong> 的原理，我们可以在同一张背景地图上，进行不同图层的绘制，最后会将所有图层效果叠加。这使得我们可以将不同效果的图形结合起来，仅举一例：<img src="/img/powermap/layer.png" alt="图层"></p>
</li>
<li><p>形状。如果画柱形图，可以选择不同的形状，如：<img src="/img/powermap/shape.png" alt="不同形状"></p>
</li>
<li><p>还有很多功能，如二维图表、文本框等等，在此不一一赘述。</p>
</li>
</ol>
]]></content>
         
         
           
             
              <breadCrumb title="Visualization" url="http://ripeconan.com/categories/Visualization/"/>
          
        </display>
    </data>
    </url>

    
    
    
  <url>
    <loc>http://ripeconan.com/2014/10/18/cumsum/</loc>
    <lastmod>2014-10-18T11:16:13.000Z</lastmod>
    <data>
        <display>
        <title>cumsum</title>
        <pubTime>2014-10-18T10:46:40.000Z</pubTime>
        
        <tag>R函数 </tag>
         
         <content><![CDATA[<h2 id="函数功能">函数功能</h2>
<p>计算向量的累积和并返回  </p>
<p>cum 系列还有另外三个函数：cumprod, cummin, cummax<br>它们的作用分别是计算向量的累积的乘积、极小值、极大值，并返回</p>
<h2 id="函数参数说明">函数参数说明</h2>
<p>cum 系列函数只有一个参数：cumsum(x), cumprod(x), cummin(x), cummax(x)  </p>
<ol>
<li>当使用 cumsum 或 cumprod 时，x 可以是数值型(numeric)或复数型(complex)的向量、矩阵、数据框，因为数值型和复数型可做求和、求积。</li>
<li>当使用 cummin 或 cummax 时，x 只能是数值型的向量、矩阵、数据框。</li>
</ol>
<h2 id="函数返回值说明">函数返回值说明</h2>
<p>当 x 是向量、矩阵时，cum 系列函数返回的是向量，因为矩阵的本质也是向量；当 x 是数据框时，返回值则仍为数据框。</p>
<h2 id="函数使用示例">函数使用示例</h2>
<p>对数值型向量求和：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">cumsum(<span class="number">1</span>:<span class="number">10</span>)</div></pre></td></tr></table></figure>



<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="preprocessor">##  [1]  1  3  6 10 15 21 28 36 45 55</span></div></pre></td></tr></table></figure>

<p>对数值型矩阵求和，结果返回仍是向量：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">cumsum(matrix(<span class="number">1</span>:<span class="number">12</span>, nrow = <span class="number">3</span>))</div></pre></td></tr></table></figure>



<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="preprocessor">##  [1]  1  3  6 10 15 21 28 36 45 55 66 78</span></div></pre></td></tr></table></figure>

<p>对数据框求和，返回结果仍然是数据框，对每个变量做操作：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">cumsum(data.frame(a = <span class="number">1</span>:<span class="number">10</span>, b = <span class="number">21</span>:<span class="number">30</span>))</div></pre></td></tr></table></figure>



<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="preprocessor">##     a   b</span></div><div class="line"><span class="preprocessor">## 1   1  21</span></div><div class="line"><span class="preprocessor">## 2   3  43</span></div><div class="line"><span class="preprocessor">## 3   6  66</span></div><div class="line"><span class="preprocessor">## 4  10  90</span></div><div class="line"><span class="preprocessor">## 5  15 115</span></div><div class="line"><span class="preprocessor">## 6  21 141</span></div><div class="line"><span class="preprocessor">## 7  28 168</span></div><div class="line"><span class="preprocessor">## 8  36 196</span></div><div class="line"><span class="preprocessor">## 9  45 225</span></div><div class="line"><span class="preprocessor">## 10 55 255</span></div></pre></td></tr></table></figure>

<p>对复数型向量，可以求积：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">x &lt;- c(<span class="number">1</span> + <span class="number">2i</span>, <span class="number">2</span> - <span class="number">3i</span>, <span class="number">5</span> + <span class="number">10i</span>); </div><div class="line">cumprod(x)</div></pre></td></tr></table></figure>



<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="preprocessor">## [1]  1+ 2i  8+ 1i 30+85i</span></div></pre></td></tr></table></figure>

<p>对数值型向量求min：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">cummin(c(<span class="number">3</span>,<span class="number">2</span>,<span class="number">5</span>,<span class="number">1</span>,<span class="number">4</span>))</div></pre></td></tr></table></figure>



<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="preprocessor">## [1] 3 2 2 1 1</span></div></pre></td></tr></table></figure>

<p>对数据框，每个变量分别求max：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">cummax(data.frame(a = c(<span class="number">3</span>,<span class="number">2</span>,<span class="number">5</span>,<span class="number">1</span>,<span class="number">4</span>), b = c(<span class="number">1</span>,<span class="number">8</span>,<span class="number">3</span>,<span class="number">6</span>,<span class="number">5</span>)))</div></pre></td></tr></table></figure>



<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="preprocessor">##   a b</span></div><div class="line"><span class="preprocessor">## 1 3 1</span></div><div class="line"><span class="preprocessor">## 2 3 8</span></div><div class="line"><span class="preprocessor">## 3 5 8</span></div><div class="line"><span class="preprocessor">## 4 5 8</span></div><div class="line"><span class="preprocessor">## 5 5 8</span></div></pre></td></tr></table></figure>

]]></content>
         
         
           
             
              <breadCrumb title="R" url="http://ripeconan.com/categories/R/"/>
          
        </display>
    </data>
    </url>

</urlset>
